{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccd04a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "import math\n",
    "\n",
    "\n",
    "import ds_layer #Dempster-Shafer layer\n",
    "import utility_layer_train #Utility layer for training\n",
    "import utility_layer_test #Utility layer for training\n",
    "import AU_imprecision\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "562deaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = np.load('/Python/HAM1000/HAM10000_npy/train_image.npy')\n",
    "train_labels = np.load('/Python/HAM1000/HAM10000_npy/train_label.npy')\n",
    "\n",
    "test_images = np.load('/Python/HAM1000/HAM10000_npy/test_image.npy')\n",
    "test_labels = np.load('/Python/HAM1000/HAM10000_npy/test_label.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6e09473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images shape: (6409, 32, 32, 3)\n",
      "Train labels shape: (6409, 1)\n",
      "Test images shape: (2003, 32, 32, 3)\n",
      "Test labels shape: (2003, 1)\n"
     ]
    }
   ],
   "source": [
    "# train_images, test_images, train_labels, test_labels = train_test_split(\n",
    "#     x_images, y_labels, test_size=0.1, random_state=42)\n",
    "\n",
    "print(\"Train images shape:\", train_images.shape)\n",
    "print(\"Train labels shape:\", train_labels.shape)\n",
    "print(\"Test images shape:\", test_images.shape)\n",
    "print(\"Test labels shape:\", test_labels.shape)\n",
    "\n",
    "# del x_images\n",
    "# del y_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "108559e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2000x200 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "random_indices = np.random.choice(train_images.shape[0], size=10, replace=False)\n",
    "fig = plt.figure(figsize=(20, 2))\n",
    "for i, index in enumerate(random_indices):\n",
    "    ax = fig.add_subplot(1, 10, i+1)\n",
    "    train_images[index] = cv2.cvtColor(train_images[index], cv2.COLOR_BGR2RGB)\n",
    "    ax.imshow(train_images[index].astype(\"uint8\"))\n",
    "    ax.axis('off')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c87fbf54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the numpy files\n",
    "labels = train_labels\n",
    "unique_labels, label_counts = np.unique(labels, return_counts=True)\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "bar_color = '#1f77b4' # Red color for the bars\n",
    "bar_width = 0.5  # Width of the bars\n",
    "bar_alpha = 0.8  # Transparency of the bars\n",
    "bar_edgecolor = 'black'  # Edge color of the bars\n",
    "\n",
    "\n",
    "bar_plot = ax.bar(unique_labels, label_counts, width=bar_width, color=bar_color, alpha=bar_alpha, edgecolor=bar_edgecolor)\n",
    "ax.set_xlabel('Labels')\n",
    "ax.set_title('Frequency Distribution')\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "ax.grid(axis='y', linestyle='')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "custom_legend = [plt.Rectangle((0, 0), 1, 1, fc=bar_color)]\n",
    "legend_labels = ['Training Set']\n",
    "ax.legend(custom_legend, legend_labels)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59c1a13e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the numpy files\n",
    "labels = test_labels\n",
    "\n",
    "unique_labels, label_counts = np.unique(labels, return_counts=True)\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "bar_color = 'lightcoral' # Red color for the bars\n",
    "bar_width = 0.5  # Width of the bars\n",
    "bar_alpha = 0.8  # Transparency of the bars\n",
    "bar_edgecolor = 'black'  # Edge color of the bars\n",
    "\n",
    "\n",
    "bar_plot = ax.bar(unique_labels, label_counts, width=bar_width, color=bar_color, alpha=bar_alpha, edgecolor=bar_edgecolor)\n",
    "ax.set_xlabel('Labels')\n",
    "ax.set_title('Frequency Distribution')\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "ax.grid(axis='y', linestyle='')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "custom_legend = [plt.Rectangle((0, 0), 1, 1, fc=bar_color)]\n",
    "legend_labels = ['Testing Set']\n",
    "ax.legend(custom_legend, legend_labels)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99592f56",
   "metadata": {},
   "source": [
    "## Probabilitstic Fitnet-4 classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adf02dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "num_classes = 7\n",
    "train_labels_one_hot = keras.utils.to_categorical(train_labels, num_classes)\n",
    "test_labels_one_hot = keras.utils.to_categorical(test_labels, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2448c4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS = train_images.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "241fe107",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0,\n",
    "    patience=0,\n",
    "    verbose=0,\n",
    "    mode='min',\n",
    "    baseline=None,\n",
    "    restore_best_weights=True,\n",
    "#     start_from_epoch=8\n",
    ")\n",
    "\n",
    "# callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(verbose=1,min_lr=0.00000001, patience=5,),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e2e5344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 32, 32, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 32, 32, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 32, 32, 48)        13872     \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 32, 32, 48)        20784     \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 32, 32, 48)       192       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 16, 16, 48)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16, 16, 48)        0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 16, 16, 80)        34640     \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 16, 16, 80)        57680     \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 16, 16, 80)        57680     \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 16, 16, 80)        57680     \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 16, 16, 80)        57680     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 16, 16, 80)       320       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 8, 8, 80)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 8, 8, 80)          0         \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 8, 8, 128)         92288     \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 8, 8, 128)         147584    \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 8, 8, 128)         147584    \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 8, 8, 128)         147584    \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 8, 8, 128)         147584    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 8, 8, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 1, 1, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 1, 1, 128)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 7)                 903       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,003,959\n",
      "Trainable params: 1,003,447\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\Python\\Anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\nadam.py:86: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "num_class=num_classes\n",
    "\n",
    "inputs = tf.keras.layers.Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
    "\n",
    "c1_1 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(inputs)\n",
    "c1_2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1_1)\n",
    "c1_3 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1_2)\n",
    "c1_4 = tf.keras.layers.Conv2D(48, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1_3)\n",
    "c1_5 = tf.keras.layers.Conv2D(48, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1_4)\n",
    "bt1 = tf.keras.layers.BatchNormalization()(c1_5)\n",
    "p1 = tf.keras.layers.MaxPooling2D((2, 2))(bt1)\n",
    "dr1 = tf.keras.layers.Dropout(0.5)(p1)\n",
    "\n",
    "\n",
    "c2_1 = tf.keras.layers.Conv2D(80, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(dr1)\n",
    "c2_2 = tf.keras.layers.Conv2D(80, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2_1)\n",
    "c2_3 = tf.keras.layers.Conv2D(80, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2_2)\n",
    "c2_4 = tf.keras.layers.Conv2D(80, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2_3)\n",
    "c2_5 = tf.keras.layers.Conv2D(80, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2_4)\n",
    "bt2 = tf.keras.layers.BatchNormalization()(c2_5)\n",
    "p2 = tf.keras.layers.MaxPooling2D((2, 2))(bt2)\n",
    "dr2 = tf.keras.layers.Dropout(0.5)(p2)\n",
    "\n",
    "c3_1 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(dr2)\n",
    "c3_2 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3_1)\n",
    "c3_3 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3_2)\n",
    "c3_4 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3_3)\n",
    "c3_5 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3_4)\n",
    "bt3 = tf.keras.layers.BatchNormalization()(c3_5)\n",
    "p3 = tf.keras.layers.MaxPooling2D((8, 8))(bt3)\n",
    "dr3 = tf.keras.layers.Dropout(0.5)(p3)\n",
    "\n",
    "flatten1=tf.keras.layers.Flatten()(dr3)\n",
    "\n",
    "outputs = tf.keras.layers.Dense(num_class, activation='softmax')(flatten1)\n",
    "\n",
    "model_p = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
    "model_p.compile(optimizer=keras.optimizers.Nadam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004), #lr=0.005\n",
    "              loss='CategoricalCrossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model_p.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4272f91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "257/257 [==============================] - ETA: 0s - loss: 1.6379 - accuracy: 0.5912\n",
      "Epoch 1: val_accuracy improved from -inf to 0.68597, saving model to /Python/BDSL49/Recognition_Processed_Compressed/DatasetNPY32x32\\cnn_checkpoint_fina\n",
      "257/257 [==============================] - 8s 18ms/step - loss: 1.6379 - accuracy: 0.5912 - val_loss: 2.3431 - val_accuracy: 0.6860\n",
      "Epoch 2/200\n",
      "256/257 [============================>.] - ETA: 0s - loss: 1.0648 - accuracy: 0.6502\n",
      "Epoch 2: val_accuracy did not improve from 0.68597\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 1.0642 - accuracy: 0.6503 - val_loss: 1.2464 - val_accuracy: 0.4698\n",
      "Epoch 3/200\n",
      "255/257 [============================>.] - ETA: 0s - loss: 0.9610 - accuracy: 0.6703\n",
      "Epoch 3: val_accuracy did not improve from 0.68597\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.9618 - accuracy: 0.6700 - val_loss: 1.1839 - val_accuracy: 0.6640\n",
      "Epoch 4/200\n",
      "257/257 [==============================] - ETA: 0s - loss: 1.0367 - accuracy: 0.6549\n",
      "Epoch 4: val_accuracy did not improve from 0.68597\n",
      "257/257 [==============================] - 4s 15ms/step - loss: 1.0367 - accuracy: 0.6549 - val_loss: 0.9874 - val_accuracy: 0.6860\n",
      "Epoch 5/200\n",
      "256/257 [============================>.] - ETA: 0s - loss: 0.9608 - accuracy: 0.6681\n",
      "Epoch 5: val_accuracy did not improve from 0.68597\n",
      "257/257 [==============================] - 4s 15ms/step - loss: 0.9640 - accuracy: 0.6678 - val_loss: 8.8944 - val_accuracy: 0.6860\n",
      "Epoch 6/200\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.9113 - accuracy: 0.6775\n",
      "Epoch 6: val_accuracy did not improve from 0.68597\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.9113 - accuracy: 0.6775 - val_loss: 0.9423 - val_accuracy: 0.6790\n",
      "Epoch 7/200\n",
      "255/257 [============================>.] - ETA: 0s - loss: 0.9083 - accuracy: 0.6784\n",
      "Epoch 7: val_accuracy did not improve from 0.68597\n",
      "257/257 [==============================] - 4s 15ms/step - loss: 0.9080 - accuracy: 0.6784 - val_loss: 1.1520 - val_accuracy: 0.6775\n",
      "Epoch 8/200\n",
      "256/257 [============================>.] - ETA: 0s - loss: 0.9307 - accuracy: 0.6737\n",
      "Epoch 8: val_accuracy did not improve from 0.68597\n",
      "257/257 [==============================] - 4s 15ms/step - loss: 0.9311 - accuracy: 0.6736 - val_loss: 1.0231 - val_accuracy: 0.6855\n",
      "Epoch 9/200\n",
      "255/257 [============================>.] - ETA: 0s - loss: 0.8901 - accuracy: 0.6792\n",
      "Epoch 9: val_accuracy improved from 0.68597 to 0.70844, saving model to /Python/BDSL49/Recognition_Processed_Compressed/DatasetNPY32x32\\cnn_checkpoint_fina\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.8897 - accuracy: 0.6794 - val_loss: 0.8399 - val_accuracy: 0.7084\n",
      "Epoch 10/200\n",
      "254/257 [============================>.] - ETA: 0s - loss: 0.8542 - accuracy: 0.6882\n",
      "Epoch 10: val_accuracy did not improve from 0.70844\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.8547 - accuracy: 0.6879 - val_loss: 0.9701 - val_accuracy: 0.7064\n",
      "Epoch 11/200\n",
      "255/257 [============================>.] - ETA: 0s - loss: 0.8520 - accuracy: 0.6941\n",
      "Epoch 11: val_accuracy did not improve from 0.70844\n",
      "257/257 [==============================] - 4s 15ms/step - loss: 0.8527 - accuracy: 0.6942 - val_loss: 0.8979 - val_accuracy: 0.6975\n",
      "Epoch 12/200\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.8398 - accuracy: 0.6951\n",
      "Epoch 12: val_accuracy improved from 0.70844 to 0.71892, saving model to /Python/BDSL49/Recognition_Processed_Compressed/DatasetNPY32x32\\cnn_checkpoint_fina\n",
      "257/257 [==============================] - 4s 15ms/step - loss: 0.8398 - accuracy: 0.6951 - val_loss: 0.8235 - val_accuracy: 0.7189\n",
      "Epoch 13/200\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.8566 - accuracy: 0.6945\n",
      "Epoch 13: val_accuracy did not improve from 0.71892\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.8566 - accuracy: 0.6945 - val_loss: 1.3808 - val_accuracy: 0.6825\n",
      "Epoch 14/200\n",
      "255/257 [============================>.] - ETA: 0s - loss: 0.8338 - accuracy: 0.6985\n",
      "Epoch 14: val_accuracy did not improve from 0.71892\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.8339 - accuracy: 0.6984 - val_loss: 0.8132 - val_accuracy: 0.6895\n",
      "Epoch 15/200\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.8079 - accuracy: 0.7042\n",
      "Epoch 15: val_accuracy did not improve from 0.71892\n",
      "257/257 [==============================] - 5s 18ms/step - loss: 0.8079 - accuracy: 0.7042 - val_loss: 0.8322 - val_accuracy: 0.7034\n",
      "Epoch 16/200\n",
      "254/257 [============================>.] - ETA: 0s - loss: 0.8050 - accuracy: 0.7057\n",
      "Epoch 16: val_accuracy did not improve from 0.71892\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.8046 - accuracy: 0.7053 - val_loss: 0.9009 - val_accuracy: 0.6600\n",
      "Epoch 17/200\n",
      "256/257 [============================>.] - ETA: 0s - loss: 0.7966 - accuracy: 0.7108\n",
      "Epoch 17: val_accuracy did not improve from 0.71892\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.7977 - accuracy: 0.7104 - val_loss: 0.9543 - val_accuracy: 0.6540\n",
      "Epoch 18/200\n",
      "255/257 [============================>.] - ETA: 0s - loss: 0.7761 - accuracy: 0.7175\n",
      "Epoch 18: val_accuracy improved from 0.71892 to 0.72941, saving model to /Python/BDSL49/Recognition_Processed_Compressed/DatasetNPY32x32\\cnn_checkpoint_fina\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.7773 - accuracy: 0.7173 - val_loss: 0.7644 - val_accuracy: 0.7294\n",
      "Epoch 19/200\n",
      "255/257 [============================>.] - ETA: 0s - loss: 0.7784 - accuracy: 0.7178\n",
      "Epoch 19: val_accuracy did not improve from 0.72941\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.7772 - accuracy: 0.7182 - val_loss: 1.0388 - val_accuracy: 0.6910\n",
      "Epoch 20/200\n",
      "255/257 [============================>.] - ETA: 0s - loss: 0.7540 - accuracy: 0.7250\n",
      "Epoch 20: val_accuracy did not improve from 0.72941\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.7540 - accuracy: 0.7255 - val_loss: 1.0560 - val_accuracy: 0.6920\n",
      "Epoch 21/200\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.7500 - accuracy: 0.7273\n",
      "Epoch 21: val_accuracy did not improve from 0.72941\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.7500 - accuracy: 0.7273 - val_loss: 0.7545 - val_accuracy: 0.7264\n",
      "Epoch 22/200\n",
      "254/257 [============================>.] - ETA: 0s - loss: 0.7401 - accuracy: 0.7271\n",
      "Epoch 22: val_accuracy did not improve from 0.72941\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.7402 - accuracy: 0.7274 - val_loss: 0.8140 - val_accuracy: 0.7284\n",
      "Epoch 23/200\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.7300 - accuracy: 0.7269\n",
      "Epoch 23: val_accuracy did not improve from 0.72941\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.7300 - accuracy: 0.7269 - val_loss: 0.7621 - val_accuracy: 0.7219\n",
      "Epoch 24/200\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.7274 - accuracy: 0.7308\n",
      "Epoch 24: val_accuracy did not improve from 0.72941\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.7274 - accuracy: 0.7308 - val_loss: 0.9669 - val_accuracy: 0.6890\n",
      "Epoch 25/200\n",
      "256/257 [============================>.] - ETA: 0s - loss: 0.7076 - accuracy: 0.7411\n",
      "Epoch 25: val_accuracy did not improve from 0.72941\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.7082 - accuracy: 0.7410 - val_loss: 0.9403 - val_accuracy: 0.6780\n",
      "Epoch 26/200\n",
      "254/257 [============================>.] - ETA: 0s - loss: 0.7181 - accuracy: 0.7335\n",
      "Epoch 26: val_accuracy improved from 0.72941 to 0.74938, saving model to /Python/BDSL49/Recognition_Processed_Compressed/DatasetNPY32x32\\cnn_checkpoint_fina\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.7191 - accuracy: 0.7332 - val_loss: 0.6812 - val_accuracy: 0.7494\n",
      "Epoch 27/200\n",
      "255/257 [============================>.] - ETA: 0s - loss: 0.8296 - accuracy: 0.7060\n",
      "Epoch 27: val_accuracy did not improve from 0.74938\n",
      "257/257 [==============================] - 5s 18ms/step - loss: 0.8286 - accuracy: 0.7062 - val_loss: 0.7914 - val_accuracy: 0.7249\n",
      "Epoch 28/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/257 [============================>.] - ETA: 0s - loss: 0.7477 - accuracy: 0.7323\n",
      "Epoch 28: val_accuracy did not improve from 0.74938\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.7472 - accuracy: 0.7323 - val_loss: 1.4948 - val_accuracy: 0.6870\n",
      "Epoch 29/200\n",
      "255/257 [============================>.] - ETA: 0s - loss: 0.7382 - accuracy: 0.7280\n",
      "Epoch 29: val_accuracy did not improve from 0.74938\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.7388 - accuracy: 0.7279 - val_loss: 0.8391 - val_accuracy: 0.7424\n",
      "Epoch 30/200\n",
      "255/257 [============================>.] - ETA: 0s - loss: 0.7102 - accuracy: 0.7407\n",
      "Epoch 30: val_accuracy improved from 0.74938 to 0.75287, saving model to /Python/BDSL49/Recognition_Processed_Compressed/DatasetNPY32x32\\cnn_checkpoint_fina\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.7115 - accuracy: 0.7405 - val_loss: 0.7091 - val_accuracy: 0.7529\n",
      "Epoch 31/200\n",
      "255/257 [============================>.] - ETA: 0s - loss: 0.7021 - accuracy: 0.7432\n",
      "Epoch 31: val_accuracy did not improve from 0.75287\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.7010 - accuracy: 0.7438 - val_loss: 0.7313 - val_accuracy: 0.7349\n",
      "Epoch 32/200\n",
      "256/257 [============================>.] - ETA: 0s - loss: 0.7045 - accuracy: 0.7416\n",
      "Epoch 32: val_accuracy did not improve from 0.75287\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.7040 - accuracy: 0.7419 - val_loss: 0.8642 - val_accuracy: 0.6905\n",
      "Epoch 33/200\n",
      "255/257 [============================>.] - ETA: 0s - loss: 0.7046 - accuracy: 0.7424\n",
      "Epoch 33: val_accuracy did not improve from 0.75287\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.7050 - accuracy: 0.7424 - val_loss: 0.7812 - val_accuracy: 0.6745\n",
      "Epoch 34/200\n",
      "255/257 [============================>.] - ETA: 0s - loss: 0.6824 - accuracy: 0.7476\n",
      "Epoch 34: val_accuracy did not improve from 0.75287\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.6833 - accuracy: 0.7475 - val_loss: 0.7566 - val_accuracy: 0.7264\n",
      "Epoch 35/200\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.6717 - accuracy: 0.7489\n",
      "Epoch 35: val_accuracy did not improve from 0.75287\n",
      "257/257 [==============================] - 5s 18ms/step - loss: 0.6717 - accuracy: 0.7489 - val_loss: 0.7172 - val_accuracy: 0.7449\n",
      "Epoch 36/200\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.6696 - accuracy: 0.7525\n",
      "Epoch 36: val_accuracy did not improve from 0.75287\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.6696 - accuracy: 0.7525 - val_loss: 1.0965 - val_accuracy: 0.7119\n",
      "Epoch 37/200\n",
      "256/257 [============================>.] - ETA: 0s - loss: 0.6548 - accuracy: 0.7548\n",
      "Epoch 37: val_accuracy did not improve from 0.75287\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.6549 - accuracy: 0.7547 - val_loss: 0.7302 - val_accuracy: 0.7269\n",
      "Epoch 38/200\n",
      "254/257 [============================>.] - ETA: 0s - loss: 0.6457 - accuracy: 0.7550\n",
      "Epoch 38: val_accuracy did not improve from 0.75287\n",
      "257/257 [==============================] - 4s 15ms/step - loss: 0.6451 - accuracy: 0.7550 - val_loss: 0.7464 - val_accuracy: 0.7389\n",
      "Epoch 39/200\n",
      "254/257 [============================>.] - ETA: 0s - loss: 0.6430 - accuracy: 0.7570\n",
      "Epoch 39: val_accuracy improved from 0.75287 to 0.77084, saving model to /Python/BDSL49/Recognition_Processed_Compressed/DatasetNPY32x32\\cnn_checkpoint_fina\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.6431 - accuracy: 0.7569 - val_loss: 0.6353 - val_accuracy: 0.7708\n",
      "Epoch 40/200\n",
      "255/257 [============================>.] - ETA: 0s - loss: 0.6473 - accuracy: 0.7597\n",
      "Epoch 40: val_accuracy did not improve from 0.77084\n",
      "257/257 [==============================] - 5s 18ms/step - loss: 0.6464 - accuracy: 0.7597 - val_loss: 0.8872 - val_accuracy: 0.7194\n",
      "Epoch 41/200\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.6291 - accuracy: 0.7694\n",
      "Epoch 41: val_accuracy did not improve from 0.77084\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.6291 - accuracy: 0.7694 - val_loss: 0.6735 - val_accuracy: 0.7609\n",
      "Epoch 42/200\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.6282 - accuracy: 0.7663\n",
      "Epoch 42: val_accuracy did not improve from 0.77084\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.6282 - accuracy: 0.7663 - val_loss: 0.6995 - val_accuracy: 0.7389\n",
      "Epoch 43/200\n",
      "255/257 [============================>.] - ETA: 0s - loss: 0.6273 - accuracy: 0.7716\n",
      "Epoch 43: val_accuracy did not improve from 0.77084\n",
      "257/257 [==============================] - 4s 15ms/step - loss: 0.6269 - accuracy: 0.7717 - val_loss: 0.6933 - val_accuracy: 0.7639\n",
      "Epoch 44/200\n",
      "254/257 [============================>.] - ETA: 0s - loss: 0.6058 - accuracy: 0.7757\n",
      "Epoch 44: val_accuracy did not improve from 0.77084\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.6077 - accuracy: 0.7750 - val_loss: 0.7610 - val_accuracy: 0.7344\n",
      "Epoch 45/200\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.6029 - accuracy: 0.7792\n",
      "Epoch 45: val_accuracy did not improve from 0.77084\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.6029 - accuracy: 0.7792 - val_loss: 0.6957 - val_accuracy: 0.7459\n",
      "Epoch 46/200\n",
      "256/257 [============================>.] - ETA: 0s - loss: 0.5960 - accuracy: 0.7791\n",
      "Epoch 46: val_accuracy did not improve from 0.77084\n",
      "257/257 [==============================] - 4s 15ms/step - loss: 0.5953 - accuracy: 0.7792 - val_loss: 0.8378 - val_accuracy: 0.7194\n",
      "Epoch 47/200\n",
      "256/257 [============================>.] - ETA: 0s - loss: 0.5916 - accuracy: 0.7806\n",
      "Epoch 47: val_accuracy did not improve from 0.77084\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.5916 - accuracy: 0.7806 - val_loss: 0.7040 - val_accuracy: 0.7434\n",
      "Epoch 48/200\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.5844 - accuracy: 0.7795\n",
      "Epoch 48: val_accuracy did not improve from 0.77084\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.5844 - accuracy: 0.7795 - val_loss: 0.6823 - val_accuracy: 0.7589\n",
      "Epoch 49/200\n",
      "254/257 [============================>.] - ETA: 0s - loss: 0.5827 - accuracy: 0.7855\n",
      "Epoch 49: val_accuracy did not improve from 0.77084\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.5835 - accuracy: 0.7844 - val_loss: 0.8183 - val_accuracy: 0.6980\n",
      "Epoch 50/200\n",
      "255/257 [============================>.] - ETA: 0s - loss: 0.5780 - accuracy: 0.7900\n",
      "Epoch 50: val_accuracy did not improve from 0.77084\n",
      "257/257 [==============================] - 4s 15ms/step - loss: 0.5780 - accuracy: 0.7900 - val_loss: 0.7513 - val_accuracy: 0.7464\n",
      "Epoch 51/200\n",
      "254/257 [============================>.] - ETA: 0s - loss: 0.5589 - accuracy: 0.7928\n",
      "Epoch 51: val_accuracy did not improve from 0.77084\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.5600 - accuracy: 0.7923 - val_loss: 1.0606 - val_accuracy: 0.7049\n",
      "Epoch 52/200\n",
      "256/257 [============================>.] - ETA: 0s - loss: 0.5451 - accuracy: 0.7962\n",
      "Epoch 52: val_accuracy did not improve from 0.77084\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.5446 - accuracy: 0.7965 - val_loss: 0.7222 - val_accuracy: 0.7424\n",
      "Epoch 53/200\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.6889 - accuracy: 0.7561\n",
      "Epoch 53: val_accuracy did not improve from 0.77084\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.6889 - accuracy: 0.7561 - val_loss: 1.5502 - val_accuracy: 0.6950\n",
      "Epoch 54/200\n",
      "256/257 [============================>.] - ETA: 0s - loss: 0.8227 - accuracy: 0.7005\n",
      "Epoch 54: val_accuracy did not improve from 0.77084\n",
      "257/257 [==============================] - 4s 15ms/step - loss: 0.8229 - accuracy: 0.7006 - val_loss: 0.7974 - val_accuracy: 0.7149\n",
      "Epoch 55/200\n",
      "256/257 [============================>.] - ETA: 0s - loss: 0.7396 - accuracy: 0.7277\n",
      "Epoch 55: val_accuracy did not improve from 0.77084\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.7402 - accuracy: 0.7276 - val_loss: 0.6901 - val_accuracy: 0.7479\n",
      "Epoch 56/200\n",
      "255/257 [============================>.] - ETA: 0s - loss: 0.6985 - accuracy: 0.7454\n",
      "Epoch 56: val_accuracy did not improve from 0.77084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257/257 [==============================] - 5s 18ms/step - loss: 0.7004 - accuracy: 0.7454 - val_loss: 0.7229 - val_accuracy: 0.7244\n",
      "Epoch 57/200\n",
      "256/257 [============================>.] - ETA: 0s - loss: 0.6682 - accuracy: 0.7580\n",
      "Epoch 57: val_accuracy did not improve from 0.77084\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.6678 - accuracy: 0.7582 - val_loss: 0.7819 - val_accuracy: 0.7174\n",
      "Epoch 58/200\n",
      "255/257 [============================>.] - ETA: 0s - loss: 0.6410 - accuracy: 0.7595\n",
      "Epoch 58: val_accuracy did not improve from 0.77084\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.6407 - accuracy: 0.7597 - val_loss: 0.6513 - val_accuracy: 0.7544\n",
      "Epoch 59/200\n",
      "256/257 [============================>.] - ETA: 0s - loss: 0.6220 - accuracy: 0.7669\n",
      "Epoch 59: val_accuracy did not improve from 0.77084\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.6220 - accuracy: 0.7667 - val_loss: 0.6739 - val_accuracy: 0.7539\n",
      "Epoch 60/200\n",
      "255/257 [============================>.] - ETA: 0s - loss: 0.5876 - accuracy: 0.7823\n",
      "Epoch 60: val_accuracy did not improve from 0.77084\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.5884 - accuracy: 0.7820 - val_loss: 0.7285 - val_accuracy: 0.7264\n",
      "Epoch 61/200\n",
      "255/257 [============================>.] - ETA: 0s - loss: 0.5707 - accuracy: 0.7862\n",
      "Epoch 61: val_accuracy did not improve from 0.77084\n",
      "257/257 [==============================] - 5s 18ms/step - loss: 0.5706 - accuracy: 0.7864 - val_loss: 0.6737 - val_accuracy: 0.7569\n",
      "Epoch 62/200\n",
      "254/257 [============================>.] - ETA: 0s - loss: 0.5663 - accuracy: 0.7857\n",
      "Epoch 62: val_accuracy did not improve from 0.77084\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.5659 - accuracy: 0.7855 - val_loss: 0.7549 - val_accuracy: 0.7164\n",
      "Epoch 63/200\n",
      "255/257 [============================>.] - ETA: 0s - loss: 0.5472 - accuracy: 0.7923\n",
      "Epoch 63: val_accuracy did not improve from 0.77084\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.5470 - accuracy: 0.7928 - val_loss: 0.6323 - val_accuracy: 0.7668\n",
      "Epoch 64/200\n",
      "255/257 [============================>.] - ETA: 0s - loss: 0.5364 - accuracy: 0.7956\n",
      "Epoch 64: val_accuracy did not improve from 0.77084\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.5352 - accuracy: 0.7959 - val_loss: 0.6951 - val_accuracy: 0.7564\n",
      "Epoch 65/200\n",
      "254/257 [============================>.] - ETA: 0s - loss: 0.5090 - accuracy: 0.8106\n",
      "Epoch 65: val_accuracy did not improve from 0.77084\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.5083 - accuracy: 0.8114 - val_loss: 0.7208 - val_accuracy: 0.7609\n",
      "Epoch 66/200\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.4988 - accuracy: 0.8106\n",
      "Epoch 66: val_accuracy did not improve from 0.77084\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.4988 - accuracy: 0.8106 - val_loss: 0.7236 - val_accuracy: 0.7504\n",
      "Epoch 67/200\n",
      "256/257 [============================>.] - ETA: 0s - loss: 0.4905 - accuracy: 0.8169\n",
      "Epoch 67: val_accuracy did not improve from 0.77084\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.4903 - accuracy: 0.8168 - val_loss: 0.7608 - val_accuracy: 0.7394\n",
      "Epoch 68/200\n",
      "256/257 [============================>.] - ETA: 0s - loss: 0.4744 - accuracy: 0.8227\n",
      "Epoch 68: val_accuracy did not improve from 0.77084\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.4741 - accuracy: 0.8229 - val_loss: 0.7191 - val_accuracy: 0.7534\n",
      "Epoch 69/200\n",
      "256/257 [============================>.] - ETA: 0s - loss: 0.4564 - accuracy: 0.8256\n",
      "Epoch 69: val_accuracy did not improve from 0.77084\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.4569 - accuracy: 0.8256 - val_loss: 0.9527 - val_accuracy: 0.7249\n",
      "Epoch 70/200\n",
      "255/257 [============================>.] - ETA: 0s - loss: 0.4578 - accuracy: 0.8264\n",
      "Epoch 70: val_accuracy did not improve from 0.77084\n",
      "257/257 [==============================] - 5s 18ms/step - loss: 0.4569 - accuracy: 0.8265 - val_loss: 0.7793 - val_accuracy: 0.7234\n",
      "Epoch 71/200\n",
      "255/257 [============================>.] - ETA: 0s - loss: 0.4227 - accuracy: 0.8409\n",
      "Epoch 71: val_accuracy improved from 0.77084 to 0.77683, saving model to /Python/BDSL49/Recognition_Processed_Compressed/DatasetNPY32x32\\cnn_checkpoint_fina\n",
      "257/257 [==============================] - 5s 18ms/step - loss: 0.4234 - accuracy: 0.8407 - val_loss: 0.6851 - val_accuracy: 0.7768\n",
      "Epoch 72/200\n",
      "256/257 [============================>.] - ETA: 0s - loss: 0.4210 - accuracy: 0.8380\n",
      "Epoch 72: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 5s 18ms/step - loss: 0.4205 - accuracy: 0.8382 - val_loss: 1.0040 - val_accuracy: 0.7259\n",
      "Epoch 73/200\n",
      "255/257 [============================>.] - ETA: 0s - loss: 0.4149 - accuracy: 0.8436\n",
      "Epoch 73: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.4147 - accuracy: 0.8437 - val_loss: 0.8239 - val_accuracy: 0.7329\n",
      "Epoch 74/200\n",
      "255/257 [============================>.] - ETA: 0s - loss: 0.4635 - accuracy: 0.8315\n",
      "Epoch 74: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.4635 - accuracy: 0.8318 - val_loss: 0.8205 - val_accuracy: 0.7419\n",
      "Epoch 75/200\n",
      "254/257 [============================>.] - ETA: 0s - loss: 0.4656 - accuracy: 0.8258\n",
      "Epoch 75: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.4657 - accuracy: 0.8260 - val_loss: 0.7304 - val_accuracy: 0.7284\n",
      "Epoch 76/200\n",
      "255/257 [============================>.] - ETA: 0s - loss: 0.4186 - accuracy: 0.8444\n",
      "Epoch 76: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.4184 - accuracy: 0.8444 - val_loss: 0.9586 - val_accuracy: 0.7109\n",
      "Epoch 77/200\n",
      "256/257 [============================>.] - ETA: 0s - loss: 0.3822 - accuracy: 0.8594\n",
      "Epoch 77: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.3818 - accuracy: 0.8596 - val_loss: 0.7972 - val_accuracy: 0.7569\n",
      "Epoch 78/200\n",
      "256/257 [============================>.] - ETA: 0s - loss: 0.3604 - accuracy: 0.8667\n",
      "Epoch 78: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.3602 - accuracy: 0.8667 - val_loss: 0.7767 - val_accuracy: 0.7534\n",
      "Epoch 79/200\n",
      "254/257 [============================>.] - ETA: 0s - loss: 0.3436 - accuracy: 0.8718\n",
      "Epoch 79: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.3433 - accuracy: 0.8719 - val_loss: 1.0568 - val_accuracy: 0.6910\n",
      "Epoch 80/200\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.3445 - accuracy: 0.8708\n",
      "Epoch 80: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.3445 - accuracy: 0.8708 - val_loss: 0.8601 - val_accuracy: 0.7574\n",
      "Epoch 81/200\n",
      "256/257 [============================>.] - ETA: 0s - loss: 0.3346 - accuracy: 0.8773\n",
      "Epoch 81: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.3349 - accuracy: 0.8774 - val_loss: 0.8258 - val_accuracy: 0.7474\n",
      "Epoch 82/200\n",
      "255/257 [============================>.] - ETA: 0s - loss: 0.3170 - accuracy: 0.8824\n",
      "Epoch 82: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.3168 - accuracy: 0.8825 - val_loss: 0.8721 - val_accuracy: 0.7334\n",
      "Epoch 83/200\n",
      "254/257 [============================>.] - ETA: 0s - loss: 0.3162 - accuracy: 0.8844\n",
      "Epoch 83: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 5s 18ms/step - loss: 0.3187 - accuracy: 0.8838 - val_loss: 1.0262 - val_accuracy: 0.7199\n",
      "Epoch 84/200\n",
      "254/257 [============================>.] - ETA: 0s - loss: 0.3003 - accuracy: 0.8857\n",
      "Epoch 84: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.3008 - accuracy: 0.8853 - val_loss: 0.8499 - val_accuracy: 0.7344\n",
      "Epoch 85/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254/257 [============================>.] - ETA: 0s - loss: 0.2857 - accuracy: 0.8901\n",
      "Epoch 85: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.2874 - accuracy: 0.8891 - val_loss: 0.8494 - val_accuracy: 0.7419\n",
      "Epoch 86/200\n",
      "256/257 [============================>.] - ETA: 0s - loss: 0.2884 - accuracy: 0.8955\n",
      "Epoch 86: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 5s 18ms/step - loss: 0.2885 - accuracy: 0.8953 - val_loss: 0.8142 - val_accuracy: 0.7494\n",
      "Epoch 87/200\n",
      "254/257 [============================>.] - ETA: 0s - loss: 0.2797 - accuracy: 0.8959\n",
      "Epoch 87: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 5s 18ms/step - loss: 0.2796 - accuracy: 0.8961 - val_loss: 0.9220 - val_accuracy: 0.7564\n",
      "Epoch 88/200\n",
      "254/257 [============================>.] - ETA: 0s - loss: 0.2808 - accuracy: 0.8989\n",
      "Epoch 88: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.2805 - accuracy: 0.8994 - val_loss: 0.9771 - val_accuracy: 0.7404\n",
      "Epoch 89/200\n",
      "256/257 [============================>.] - ETA: 0s - loss: 0.2611 - accuracy: 0.9020\n",
      "Epoch 89: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 5s 18ms/step - loss: 0.2609 - accuracy: 0.9022 - val_loss: 0.9909 - val_accuracy: 0.7604\n",
      "Epoch 90/200\n",
      "254/257 [============================>.] - ETA: 0s - loss: 0.2484 - accuracy: 0.9117\n",
      "Epoch 90: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.2494 - accuracy: 0.9115 - val_loss: 0.9342 - val_accuracy: 0.7649\n",
      "Epoch 91/200\n",
      "255/257 [============================>.] - ETA: 0s - loss: 0.2618 - accuracy: 0.9067\n",
      "Epoch 91: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 5s 18ms/step - loss: 0.2615 - accuracy: 0.9067 - val_loss: 1.0221 - val_accuracy: 0.7264\n",
      "Epoch 92/200\n",
      "255/257 [============================>.] - ETA: 0s - loss: 0.2227 - accuracy: 0.9159\n",
      "Epoch 92: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 5s 18ms/step - loss: 0.2224 - accuracy: 0.9159 - val_loss: 0.9527 - val_accuracy: 0.7479\n",
      "Epoch 93/200\n",
      "256/257 [============================>.] - ETA: 0s - loss: 0.2230 - accuracy: 0.9189\n",
      "Epoch 93: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.2235 - accuracy: 0.9186 - val_loss: 0.9357 - val_accuracy: 0.7319\n",
      "Epoch 94/200\n",
      "256/257 [============================>.] - ETA: 0s - loss: 0.2225 - accuracy: 0.9191\n",
      "Epoch 94: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.2225 - accuracy: 0.9190 - val_loss: 0.9863 - val_accuracy: 0.7579\n",
      "Epoch 95/200\n",
      "255/257 [============================>.] - ETA: 0s - loss: 0.2123 - accuracy: 0.9209\n",
      "Epoch 95: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.2122 - accuracy: 0.9212 - val_loss: 1.1503 - val_accuracy: 0.7434\n",
      "Epoch 96/200\n",
      "256/257 [============================>.] - ETA: 0s - loss: 0.2060 - accuracy: 0.9237\n",
      "Epoch 96: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.2059 - accuracy: 0.9239 - val_loss: 0.9903 - val_accuracy: 0.7249\n",
      "Epoch 97/200\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.1986 - accuracy: 0.9281\n",
      "Epoch 97: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.1986 - accuracy: 0.9281 - val_loss: 1.0007 - val_accuracy: 0.7599\n",
      "Epoch 98/200\n",
      "255/257 [============================>.] - ETA: 0s - loss: 0.1971 - accuracy: 0.9297\n",
      "Epoch 98: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.1974 - accuracy: 0.9296 - val_loss: 1.0745 - val_accuracy: 0.7444\n",
      "Epoch 99/200\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.2005 - accuracy: 0.9306\n",
      "Epoch 99: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.2005 - accuracy: 0.9306 - val_loss: 1.0080 - val_accuracy: 0.7429\n",
      "Epoch 100/200\n",
      "254/257 [============================>.] - ETA: 0s - loss: 0.1883 - accuracy: 0.9285\n",
      "Epoch 100: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.1882 - accuracy: 0.9282 - val_loss: 1.0984 - val_accuracy: 0.6970\n",
      "Epoch 101/200\n",
      "256/257 [============================>.] - ETA: 0s - loss: 0.1758 - accuracy: 0.9362\n",
      "Epoch 101: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.1759 - accuracy: 0.9360 - val_loss: 1.1718 - val_accuracy: 0.7519\n",
      "Epoch 102/200\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.1759 - accuracy: 0.9377\n",
      "Epoch 102: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.1759 - accuracy: 0.9377 - val_loss: 1.2235 - val_accuracy: 0.7414\n",
      "Epoch 103/200\n",
      "254/257 [============================>.] - ETA: 0s - loss: 0.2001 - accuracy: 0.9307\n",
      "Epoch 103: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.2007 - accuracy: 0.9306 - val_loss: 1.1528 - val_accuracy: 0.7594\n",
      "Epoch 104/200\n",
      "256/257 [============================>.] - ETA: 0s - loss: 0.1661 - accuracy: 0.9389\n",
      "Epoch 104: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.1661 - accuracy: 0.9388 - val_loss: 1.1058 - val_accuracy: 0.7289\n",
      "Epoch 105/200\n",
      "256/257 [============================>.] - ETA: 0s - loss: 0.1710 - accuracy: 0.9372\n",
      "Epoch 105: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 5s 18ms/step - loss: 0.1709 - accuracy: 0.9373 - val_loss: 1.0033 - val_accuracy: 0.7589\n",
      "Epoch 106/200\n",
      "254/257 [============================>.] - ETA: 0s - loss: 0.1556 - accuracy: 0.9461\n",
      "Epoch 106: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.1562 - accuracy: 0.9462 - val_loss: 1.2193 - val_accuracy: 0.7459\n",
      "Epoch 107/200\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.1533 - accuracy: 0.9452\n",
      "Epoch 107: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.1533 - accuracy: 0.9452 - val_loss: 1.2384 - val_accuracy: 0.7634\n",
      "Epoch 108/200\n",
      "256/257 [============================>.] - ETA: 0s - loss: 0.1455 - accuracy: 0.9491\n",
      "Epoch 108: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.1453 - accuracy: 0.9491 - val_loss: 1.2506 - val_accuracy: 0.7444\n",
      "Epoch 109/200\n",
      "256/257 [============================>.] - ETA: 0s - loss: 0.1458 - accuracy: 0.9453\n",
      "Epoch 109: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.1456 - accuracy: 0.9454 - val_loss: 1.2399 - val_accuracy: 0.7344\n",
      "Epoch 110/200\n",
      "256/257 [============================>.] - ETA: 0s - loss: 0.1340 - accuracy: 0.9523\n",
      "Epoch 110: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.1341 - accuracy: 0.9523 - val_loss: 1.1570 - val_accuracy: 0.7259\n",
      "Epoch 111/200\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.1324 - accuracy: 0.9519\n",
      "Epoch 111: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.1324 - accuracy: 0.9519 - val_loss: 1.2078 - val_accuracy: 0.7079\n",
      "Epoch 112/200\n",
      "255/257 [============================>.] - ETA: 0s - loss: 0.1466 - accuracy: 0.9498\n",
      "Epoch 112: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.1482 - accuracy: 0.9494 - val_loss: 1.2316 - val_accuracy: 0.7279\n",
      "Epoch 113/200\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.1374 - accuracy: 0.9543\n",
      "Epoch 113: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 15ms/step - loss: 0.1374 - accuracy: 0.9543 - val_loss: 1.4620 - val_accuracy: 0.7414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114/200\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.1334 - accuracy: 0.9558\n",
      "Epoch 114: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.1334 - accuracy: 0.9558 - val_loss: 1.3093 - val_accuracy: 0.7419\n",
      "Epoch 115/200\n",
      "256/257 [============================>.] - ETA: 0s - loss: 0.1328 - accuracy: 0.9542\n",
      "Epoch 115: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 15ms/step - loss: 0.1333 - accuracy: 0.9541 - val_loss: 1.3048 - val_accuracy: 0.7404\n",
      "Epoch 116/200\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.1309 - accuracy: 0.9552\n",
      "Epoch 116: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.1309 - accuracy: 0.9552 - val_loss: 1.1591 - val_accuracy: 0.7299\n",
      "Epoch 117/200\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.1272 - accuracy: 0.9543\n",
      "Epoch 117: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.1272 - accuracy: 0.9543 - val_loss: 1.7280 - val_accuracy: 0.6775\n",
      "Epoch 118/200\n",
      "254/257 [============================>.] - ETA: 0s - loss: 0.1595 - accuracy: 0.9427\n",
      "Epoch 118: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.1601 - accuracy: 0.9424 - val_loss: 1.2803 - val_accuracy: 0.7149\n",
      "Epoch 119/200\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.1401 - accuracy: 0.9507\n",
      "Epoch 119: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.1401 - accuracy: 0.9507 - val_loss: 1.2782 - val_accuracy: 0.7374\n",
      "Epoch 120/200\n",
      "255/257 [============================>.] - ETA: 0s - loss: 0.1074 - accuracy: 0.9616\n",
      "Epoch 120: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 15ms/step - loss: 0.1081 - accuracy: 0.9613 - val_loss: 1.4764 - val_accuracy: 0.7189\n",
      "Epoch 121/200\n",
      "256/257 [============================>.] - ETA: 0s - loss: 0.1159 - accuracy: 0.9588\n",
      "Epoch 121: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.1163 - accuracy: 0.9585 - val_loss: 1.3871 - val_accuracy: 0.6835\n",
      "Epoch 122/200\n",
      "254/257 [============================>.] - ETA: 0s - loss: 0.1265 - accuracy: 0.9554\n",
      "Epoch 122: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.1264 - accuracy: 0.9557 - val_loss: 1.5883 - val_accuracy: 0.7244\n",
      "Epoch 123/200\n",
      "255/257 [============================>.] - ETA: 0s - loss: 0.0938 - accuracy: 0.9672\n",
      "Epoch 123: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.0934 - accuracy: 0.9674 - val_loss: 1.3856 - val_accuracy: 0.7614\n",
      "Epoch 124/200\n",
      "256/257 [============================>.] - ETA: 0s - loss: 0.1063 - accuracy: 0.9617\n",
      "Epoch 124: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.1065 - accuracy: 0.9616 - val_loss: 1.5968 - val_accuracy: 0.7279\n",
      "Epoch 125/200\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.1143 - accuracy: 0.9627\n",
      "Epoch 125: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 5s 19ms/step - loss: 0.1143 - accuracy: 0.9627 - val_loss: 1.4397 - val_accuracy: 0.7314\n",
      "Epoch 126/200\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.1015 - accuracy: 0.9633\n",
      "Epoch 126: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 5s 18ms/step - loss: 0.1015 - accuracy: 0.9633 - val_loss: 1.6697 - val_accuracy: 0.7234\n",
      "Epoch 127/200\n",
      "256/257 [============================>.] - ETA: 0s - loss: 0.1036 - accuracy: 0.9616\n",
      "Epoch 127: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 5s 19ms/step - loss: 0.1037 - accuracy: 0.9615 - val_loss: 1.5444 - val_accuracy: 0.7234\n",
      "Epoch 128/200\n",
      "254/257 [============================>.] - ETA: 0s - loss: 0.1171 - accuracy: 0.9584\n",
      "Epoch 128: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.1184 - accuracy: 0.9580 - val_loss: 1.2647 - val_accuracy: 0.7379\n",
      "Epoch 129/200\n",
      "254/257 [============================>.] - ETA: 0s - loss: 0.0955 - accuracy: 0.9666\n",
      "Epoch 129: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.0956 - accuracy: 0.9665 - val_loss: 1.3004 - val_accuracy: 0.7519\n",
      "Epoch 130/200\n",
      "256/257 [============================>.] - ETA: 0s - loss: 0.0966 - accuracy: 0.9680\n",
      "Epoch 130: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.0970 - accuracy: 0.9679 - val_loss: 1.4504 - val_accuracy: 0.7139\n",
      "Epoch 131/200\n",
      "255/257 [============================>.] - ETA: 0s - loss: 0.1001 - accuracy: 0.9625\n",
      "Epoch 131: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.1004 - accuracy: 0.9626 - val_loss: 1.4124 - val_accuracy: 0.7379\n",
      "Epoch 132/200\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.1049 - accuracy: 0.9640\n",
      "Epoch 132: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 5s 18ms/step - loss: 0.1049 - accuracy: 0.9640 - val_loss: 1.2935 - val_accuracy: 0.7589\n",
      "Epoch 133/200\n",
      "255/257 [============================>.] - ETA: 0s - loss: 0.0877 - accuracy: 0.9704\n",
      "Epoch 133: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.0875 - accuracy: 0.9705 - val_loss: 1.2905 - val_accuracy: 0.7678\n",
      "Epoch 134/200\n",
      "256/257 [============================>.] - ETA: 0s - loss: 0.0930 - accuracy: 0.9669\n",
      "Epoch 134: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.0932 - accuracy: 0.9668 - val_loss: 1.5908 - val_accuracy: 0.7384\n",
      "Epoch 135/200\n",
      "256/257 [============================>.] - ETA: 0s - loss: 0.1063 - accuracy: 0.9622\n",
      "Epoch 135: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.1062 - accuracy: 0.9622 - val_loss: 1.2965 - val_accuracy: 0.7579\n",
      "Epoch 136/200\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.0899 - accuracy: 0.9697\n",
      "Epoch 136: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.0899 - accuracy: 0.9697 - val_loss: 1.3269 - val_accuracy: 0.7464\n",
      "Epoch 137/200\n",
      "255/257 [============================>.] - ETA: 0s - loss: 0.0919 - accuracy: 0.9705\n",
      "Epoch 137: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.0919 - accuracy: 0.9705 - val_loss: 1.4344 - val_accuracy: 0.7499\n",
      "Epoch 138/200\n",
      "255/257 [============================>.] - ETA: 0s - loss: 0.0924 - accuracy: 0.9689\n",
      "Epoch 138: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.0931 - accuracy: 0.9688 - val_loss: 1.3755 - val_accuracy: 0.7534\n",
      "Epoch 139/200\n",
      "254/257 [============================>.] - ETA: 0s - loss: 0.0841 - accuracy: 0.9710\n",
      "Epoch 139: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 15ms/step - loss: 0.0837 - accuracy: 0.9711 - val_loss: 1.3640 - val_accuracy: 0.7449\n",
      "Epoch 140/200\n",
      "255/257 [============================>.] - ETA: 0s - loss: 0.0793 - accuracy: 0.9741\n",
      "Epoch 140: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.0790 - accuracy: 0.9743 - val_loss: 1.6303 - val_accuracy: 0.7254\n",
      "Epoch 141/200\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.0880 - accuracy: 0.9730\n",
      "Epoch 141: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.0880 - accuracy: 0.9730 - val_loss: 1.4518 - val_accuracy: 0.7259\n",
      "Epoch 142/200\n",
      "256/257 [============================>.] - ETA: 0s - loss: 0.0816 - accuracy: 0.9723\n",
      "Epoch 142: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.0815 - accuracy: 0.9724 - val_loss: 1.3894 - val_accuracy: 0.7479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 143/200\n",
      "255/257 [============================>.] - ETA: 0s - loss: 0.0794 - accuracy: 0.9733\n",
      "Epoch 143: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.0796 - accuracy: 0.9733 - val_loss: 1.5336 - val_accuracy: 0.7134\n",
      "Epoch 144/200\n",
      "255/257 [============================>.] - ETA: 0s - loss: 0.0825 - accuracy: 0.9727\n",
      "Epoch 144: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.0821 - accuracy: 0.9729 - val_loss: 1.3611 - val_accuracy: 0.7509\n",
      "Epoch 145/200\n",
      "254/257 [============================>.] - ETA: 0s - loss: 0.0893 - accuracy: 0.9729\n",
      "Epoch 145: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.0889 - accuracy: 0.9730 - val_loss: 1.5264 - val_accuracy: 0.7224\n",
      "Epoch 146/200\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.0743 - accuracy: 0.9749\n",
      "Epoch 146: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 5s 18ms/step - loss: 0.0743 - accuracy: 0.9749 - val_loss: 1.5277 - val_accuracy: 0.7534\n",
      "Epoch 147/200\n",
      "256/257 [============================>.] - ETA: 0s - loss: 0.1791 - accuracy: 0.9434\n",
      "Epoch 147: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.1792 - accuracy: 0.9434 - val_loss: 1.0386 - val_accuracy: 0.7449\n",
      "Epoch 148/200\n",
      "254/257 [============================>.] - ETA: 0s - loss: 0.1319 - accuracy: 0.9557\n",
      "Epoch 148: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.1311 - accuracy: 0.9560 - val_loss: 1.1516 - val_accuracy: 0.7589\n",
      "Epoch 149/200\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.0977 - accuracy: 0.9660\n",
      "Epoch 149: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.0977 - accuracy: 0.9660 - val_loss: 1.3688 - val_accuracy: 0.7404\n",
      "Epoch 150/200\n",
      "256/257 [============================>.] - ETA: 0s - loss: 0.0768 - accuracy: 0.9750\n",
      "Epoch 150: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.0767 - accuracy: 0.9750 - val_loss: 1.5144 - val_accuracy: 0.7519\n",
      "Epoch 151/200\n",
      "256/257 [============================>.] - ETA: 0s - loss: 0.0777 - accuracy: 0.9741\n",
      "Epoch 151: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.0776 - accuracy: 0.9741 - val_loss: 1.4328 - val_accuracy: 0.7394\n",
      "Epoch 152/200\n",
      "256/257 [============================>.] - ETA: 0s - loss: 0.0791 - accuracy: 0.9755\n",
      "Epoch 152: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.0790 - accuracy: 0.9755 - val_loss: 1.4380 - val_accuracy: 0.7609\n",
      "Epoch 153/200\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.0828 - accuracy: 0.9707\n",
      "Epoch 153: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.0828 - accuracy: 0.9707 - val_loss: 1.3724 - val_accuracy: 0.7374\n",
      "Epoch 154/200\n",
      "256/257 [============================>.] - ETA: 0s - loss: 0.0653 - accuracy: 0.9770\n",
      "Epoch 154: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.0653 - accuracy: 0.9769 - val_loss: 1.5172 - val_accuracy: 0.7574\n",
      "Epoch 155/200\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.0619 - accuracy: 0.9786\n",
      "Epoch 155: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.0619 - accuracy: 0.9786 - val_loss: 1.4629 - val_accuracy: 0.7454\n",
      "Epoch 156/200\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.0650 - accuracy: 0.9771\n",
      "Epoch 156: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.0650 - accuracy: 0.9771 - val_loss: 1.3591 - val_accuracy: 0.7544\n",
      "Epoch 157/200\n",
      "256/257 [============================>.] - ETA: 0s - loss: 0.0628 - accuracy: 0.9791\n",
      "Epoch 157: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.0629 - accuracy: 0.9789 - val_loss: 1.5426 - val_accuracy: 0.7474\n",
      "Epoch 158/200\n",
      "254/257 [============================>.] - ETA: 0s - loss: 0.0710 - accuracy: 0.9769\n",
      "Epoch 158: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 5s 18ms/step - loss: 0.0708 - accuracy: 0.9769 - val_loss: 1.7204 - val_accuracy: 0.7284\n",
      "Epoch 159/200\n",
      "254/257 [============================>.] - ETA: 0s - loss: 0.0815 - accuracy: 0.9739\n",
      "Epoch 159: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.0815 - accuracy: 0.9736 - val_loss: 1.4712 - val_accuracy: 0.7354\n",
      "Epoch 160/200\n",
      "254/257 [============================>.] - ETA: 0s - loss: 0.0746 - accuracy: 0.9757\n",
      "Epoch 160: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 5s 18ms/step - loss: 0.0742 - accuracy: 0.9758 - val_loss: 1.4227 - val_accuracy: 0.7549\n",
      "Epoch 161/200\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.0727 - accuracy: 0.9747\n",
      "Epoch 161: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.0727 - accuracy: 0.9747 - val_loss: 1.5355 - val_accuracy: 0.7469\n",
      "Epoch 162/200\n",
      "256/257 [============================>.] - ETA: 0s - loss: 0.0544 - accuracy: 0.9811\n",
      "Epoch 162: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.0550 - accuracy: 0.9808 - val_loss: 1.5225 - val_accuracy: 0.7509\n",
      "Epoch 163/200\n",
      "255/257 [============================>.] - ETA: 0s - loss: 0.0568 - accuracy: 0.9796\n",
      "Epoch 163: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 15ms/step - loss: 0.0566 - accuracy: 0.9797 - val_loss: 1.4968 - val_accuracy: 0.7499\n",
      "Epoch 164/200\n",
      "256/257 [============================>.] - ETA: 0s - loss: 0.0709 - accuracy: 0.9759\n",
      "Epoch 164: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.0709 - accuracy: 0.9758 - val_loss: 1.4412 - val_accuracy: 0.7559\n",
      "Epoch 165/200\n",
      "255/257 [============================>.] - ETA: 0s - loss: 0.0691 - accuracy: 0.9785\n",
      "Epoch 165: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.0688 - accuracy: 0.9786 - val_loss: 1.4977 - val_accuracy: 0.6865\n",
      "Epoch 166/200\n",
      "254/257 [============================>.] - ETA: 0s - loss: 0.0725 - accuracy: 0.9767\n",
      "Epoch 166: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.0720 - accuracy: 0.9769 - val_loss: 1.3855 - val_accuracy: 0.7419\n",
      "Epoch 167/200\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.0620 - accuracy: 0.9808\n",
      "Epoch 167: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.0620 - accuracy: 0.9808 - val_loss: 1.4896 - val_accuracy: 0.7129\n",
      "Epoch 168/200\n",
      "256/257 [============================>.] - ETA: 0s - loss: 0.0580 - accuracy: 0.9820\n",
      "Epoch 168: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.0580 - accuracy: 0.9821 - val_loss: 1.6333 - val_accuracy: 0.7289\n",
      "Epoch 169/200\n",
      "256/257 [============================>.] - ETA: 0s - loss: 0.0561 - accuracy: 0.9812\n",
      "Epoch 169: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.0562 - accuracy: 0.9811 - val_loss: 1.4334 - val_accuracy: 0.7579\n",
      "Epoch 170/200\n",
      "254/257 [============================>.] - ETA: 0s - loss: 0.0621 - accuracy: 0.9802\n",
      "Epoch 170: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.0623 - accuracy: 0.9799 - val_loss: 1.6829 - val_accuracy: 0.7534\n",
      "Epoch 171/200\n",
      "254/257 [============================>.] - ETA: 0s - loss: 0.0647 - accuracy: 0.9787\n",
      "Epoch 171: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.0645 - accuracy: 0.9788 - val_loss: 1.5602 - val_accuracy: 0.7554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172/200\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.0601 - accuracy: 0.9800\n",
      "Epoch 172: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.0601 - accuracy: 0.9800 - val_loss: 1.4790 - val_accuracy: 0.7324\n",
      "Epoch 173/200\n",
      "254/257 [============================>.] - ETA: 0s - loss: 0.0719 - accuracy: 0.9783\n",
      "Epoch 173: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.0726 - accuracy: 0.9782 - val_loss: 1.6122 - val_accuracy: 0.7399\n",
      "Epoch 174/200\n",
      "256/257 [============================>.] - ETA: 0s - loss: 0.0571 - accuracy: 0.9812\n",
      "Epoch 174: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.0574 - accuracy: 0.9811 - val_loss: 1.5782 - val_accuracy: 0.7374\n",
      "Epoch 175/200\n",
      "253/257 [============================>.] - ETA: 0s - loss: 0.0714 - accuracy: 0.9777\n",
      "Epoch 175: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 15ms/step - loss: 0.0715 - accuracy: 0.9777 - val_loss: 1.4052 - val_accuracy: 0.7629\n",
      "Epoch 176/200\n",
      "256/257 [============================>.] - ETA: 0s - loss: 0.0682 - accuracy: 0.9814\n",
      "Epoch 176: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 15ms/step - loss: 0.0687 - accuracy: 0.9813 - val_loss: 1.4126 - val_accuracy: 0.7424\n",
      "Epoch 177/200\n",
      "254/257 [============================>.] - ETA: 0s - loss: 0.0615 - accuracy: 0.9797\n",
      "Epoch 177: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.0619 - accuracy: 0.9796 - val_loss: 1.4876 - val_accuracy: 0.7419\n",
      "Epoch 178/200\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.0494 - accuracy: 0.9850\n",
      "Epoch 178: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.0494 - accuracy: 0.9850 - val_loss: 1.5192 - val_accuracy: 0.7289\n",
      "Epoch 179/200\n",
      "254/257 [============================>.] - ETA: 0s - loss: 0.0522 - accuracy: 0.9819\n",
      "Epoch 179: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.0521 - accuracy: 0.9819 - val_loss: 1.4419 - val_accuracy: 0.7554\n",
      "Epoch 180/200\n",
      "255/257 [============================>.] - ETA: 0s - loss: 0.0556 - accuracy: 0.9823\n",
      "Epoch 180: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.0555 - accuracy: 0.9822 - val_loss: 1.6180 - val_accuracy: 0.7429\n",
      "Epoch 181/200\n",
      "255/257 [============================>.] - ETA: 0s - loss: 0.0570 - accuracy: 0.9802\n",
      "Epoch 181: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.0568 - accuracy: 0.9803 - val_loss: 1.6542 - val_accuracy: 0.7564\n",
      "Epoch 182/200\n",
      "256/257 [============================>.] - ETA: 0s - loss: 0.0679 - accuracy: 0.9772\n",
      "Epoch 182: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.0678 - accuracy: 0.9772 - val_loss: 1.3202 - val_accuracy: 0.7304\n",
      "Epoch 183/200\n",
      "254/257 [============================>.] - ETA: 0s - loss: 0.0602 - accuracy: 0.9811\n",
      "Epoch 183: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.0597 - accuracy: 0.9813 - val_loss: 1.5704 - val_accuracy: 0.7414\n",
      "Epoch 184/200\n",
      "255/257 [============================>.] - ETA: 0s - loss: 0.0492 - accuracy: 0.9838\n",
      "Epoch 184: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.0490 - accuracy: 0.9839 - val_loss: 1.5308 - val_accuracy: 0.7299\n",
      "Epoch 185/200\n",
      "256/257 [============================>.] - ETA: 0s - loss: 0.0641 - accuracy: 0.9795\n",
      "Epoch 185: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.0640 - accuracy: 0.9796 - val_loss: 1.7196 - val_accuracy: 0.7589\n",
      "Epoch 186/200\n",
      "255/257 [============================>.] - ETA: 0s - loss: 0.0505 - accuracy: 0.9827\n",
      "Epoch 186: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.0504 - accuracy: 0.9828 - val_loss: 1.5357 - val_accuracy: 0.7394\n",
      "Epoch 187/200\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.0545 - accuracy: 0.9822\n",
      "Epoch 187: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.0545 - accuracy: 0.9822 - val_loss: 1.4812 - val_accuracy: 0.7424\n",
      "Epoch 188/200\n",
      "254/257 [============================>.] - ETA: 0s - loss: 0.0609 - accuracy: 0.9803\n",
      "Epoch 188: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.0611 - accuracy: 0.9803 - val_loss: 1.7500 - val_accuracy: 0.7494\n",
      "Epoch 189/200\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.0545 - accuracy: 0.9831\n",
      "Epoch 189: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 15ms/step - loss: 0.0545 - accuracy: 0.9831 - val_loss: 1.5075 - val_accuracy: 0.7259\n",
      "Epoch 190/200\n",
      "256/257 [============================>.] - ETA: 0s - loss: 0.0481 - accuracy: 0.9847\n",
      "Epoch 190: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.0481 - accuracy: 0.9847 - val_loss: 1.5792 - val_accuracy: 0.7604\n",
      "Epoch 191/200\n",
      "256/257 [============================>.] - ETA: 0s - loss: 0.0456 - accuracy: 0.9839\n",
      "Epoch 191: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 5s 18ms/step - loss: 0.0455 - accuracy: 0.9839 - val_loss: 1.6395 - val_accuracy: 0.7424\n",
      "Epoch 192/200\n",
      "256/257 [============================>.] - ETA: 0s - loss: 0.0397 - accuracy: 0.9859\n",
      "Epoch 192: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.0399 - accuracy: 0.9858 - val_loss: 1.8389 - val_accuracy: 0.7254\n",
      "Epoch 193/200\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.0586 - accuracy: 0.9794\n",
      "Epoch 193: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.0586 - accuracy: 0.9794 - val_loss: 1.6781 - val_accuracy: 0.7309\n",
      "Epoch 194/200\n",
      "256/257 [============================>.] - ETA: 0s - loss: 0.0469 - accuracy: 0.9855\n",
      "Epoch 194: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.0469 - accuracy: 0.9855 - val_loss: 1.5159 - val_accuracy: 0.7509\n",
      "Epoch 195/200\n",
      "256/257 [============================>.] - ETA: 0s - loss: 0.0422 - accuracy: 0.9883\n",
      "Epoch 195: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.0427 - accuracy: 0.9881 - val_loss: 1.6421 - val_accuracy: 0.7424\n",
      "Epoch 196/200\n",
      "255/257 [============================>.] - ETA: 0s - loss: 0.0609 - accuracy: 0.9796\n",
      "Epoch 196: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.0608 - accuracy: 0.9796 - val_loss: 1.5571 - val_accuracy: 0.7509\n",
      "Epoch 197/200\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.0527 - accuracy: 0.9822\n",
      "Epoch 197: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.0527 - accuracy: 0.9822 - val_loss: 1.8120 - val_accuracy: 0.7129\n",
      "Epoch 198/200\n",
      "254/257 [============================>.] - ETA: 0s - loss: 0.0498 - accuracy: 0.9825\n",
      "Epoch 198: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.0498 - accuracy: 0.9824 - val_loss: 1.4924 - val_accuracy: 0.7664\n",
      "Epoch 199/200\n",
      "256/257 [============================>.] - ETA: 0s - loss: 0.0516 - accuracy: 0.9817\n",
      "Epoch 199: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.0515 - accuracy: 0.9817 - val_loss: 1.5485 - val_accuracy: 0.7509\n",
      "Epoch 200/200\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.0497 - accuracy: 0.9831\n",
      "Epoch 200: val_accuracy did not improve from 0.77683\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.0497 - accuracy: 0.9831 - val_loss: 1.3616 - val_accuracy: 0.7529\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x267c44b7eb8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = '/Python/BDSL49/Recognition_Processed_Compressed/DatasetNPY32x32/cnn_checkpoint_fina'#please define our own filepath to save the weights of the probabilistic FitNet-4 classifier\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath, monitor='val_accuracy', verbose=1,\n",
    "    save_best_only=True, save_weights_only=True,\n",
    "    save_frequency=1)\n",
    "\n",
    "model_p.fit(train_images, train_labels_one_hot, batch_size=25, epochs=200, verbose=1, callbacks=[checkpoint_callback], validation_data=(test_images, test_labels_one_hot), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91c05d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257/257 [==============================] - 1s 3ms/step - loss: 0.3017 - accuracy: 0.8872\n",
      "81/81 [==============================] - 0s 4ms/step - loss: 0.6851 - accuracy: 0.7768\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6851345300674438, 0.7768347263336182]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_p.load_weights('/Python/BDSL49/Recognition_Processed_Compressed/DatasetNPY32x32/cnn_checkpoint_fina')\n",
    "model_p.evaluate(train_images, train_labels_one_hot, batch_size=25, verbose=1)\n",
    "model_p.evaluate(test_images, test_labels_one_hot, batch_size=25, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77ce8f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 15). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Python/BDSL49/Recognition_Processed_Compressed/DatasetNPY32x32/cnn_checkpoint_finasaved_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Python/BDSL49/Recognition_Processed_Compressed/DatasetNPY32x32/cnn_checkpoint_finasaved_model/assets\n"
     ]
    }
   ],
   "source": [
    "# Save model in HDF5 format (.h5)\n",
    "model_p.save(filepath + 'model.h5')\n",
    "tf.saved_model.save(model_p, filepath + 'saved_model/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e9c666",
   "metadata": {},
   "source": [
    "## Evidential - FitNet4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3d8216c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "32\n",
      "7\n",
      "1024\n"
     ]
    }
   ],
   "source": [
    "print(IMG_WIDTH)\n",
    "print(IMG_HEIGHT)\n",
    "print(num_class)\n",
    "inputs_pixels = IMG_WIDTH * IMG_HEIGHT\n",
    "print(inputs_pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7a31fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " conv2d_15 (Conv2D)          (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " conv2d_16 (Conv2D)          (None, 32, 32, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_17 (Conv2D)          (None, 32, 32, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_18 (Conv2D)          (None, 32, 32, 48)        13872     \n",
      "                                                                 \n",
      " conv2d_19 (Conv2D)          (None, 32, 32, 48)        20784     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 32, 32, 48)       192       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 16, 16, 48)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 16, 16, 48)        0         \n",
      "                                                                 \n",
      " conv2d_20 (Conv2D)          (None, 16, 16, 80)        34640     \n",
      "                                                                 \n",
      " conv2d_21 (Conv2D)          (None, 16, 16, 80)        57680     \n",
      "                                                                 \n",
      " conv2d_22 (Conv2D)          (None, 16, 16, 80)        57680     \n",
      "                                                                 \n",
      " conv2d_23 (Conv2D)          (None, 16, 16, 80)        57680     \n",
      "                                                                 \n",
      " conv2d_24 (Conv2D)          (None, 16, 16, 80)        57680     \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 16, 16, 80)       320       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 8, 8, 80)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 8, 8, 80)          0         \n",
      "                                                                 \n",
      " conv2d_25 (Conv2D)          (None, 8, 8, 128)         92288     \n",
      "                                                                 \n",
      " conv2d_26 (Conv2D)          (None, 8, 8, 128)         147584    \n",
      "                                                                 \n",
      " conv2d_27 (Conv2D)          (None, 8, 8, 128)         147584    \n",
      "                                                                 \n",
      " conv2d_28 (Conv2D)          (None, 8, 8, 128)         147584    \n",
      "                                                                 \n",
      " conv2d_29 (Conv2D)          (None, 8, 8, 128)         147584    \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 8, 8, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 1, 1, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 1, 1, 128)         0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 128)               0         \n",
      "                                                                 \n",
      " ds1 (DS1)                   (None, 200)               25600     \n",
      "                                                                 \n",
      " ds1_activate (DS1_activate)  (None, 200)              400       \n",
      "                                                                 \n",
      " ds2 (DS2)                   (None, 200, 7)            1400      \n",
      "                                                                 \n",
      " ds2_omega (DS2_omega)       (None, 200, 8)            0         \n",
      "                                                                 \n",
      " ds3__dempster (DS3_Dempster  (None, 8)                0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " ds3_normalize (DS3_normaliz  (None, 8)                0         \n",
      " e)                                                              \n",
      "                                                                 \n",
      " dm (DM)                     (None, 7)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,030,456\n",
      "Trainable params: 1,029,944\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\Python\\Anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\nadam.py:86: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.keras.layers.Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
    "inputs_pixels = IMG_WIDTH * IMG_HEIGHT\n",
    "prototypes = 200\n",
    "prototypes=200\n",
    "\n",
    "inputs = tf.keras.layers.Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
    "\n",
    "c1_1 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(inputs)\n",
    "c1_2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1_1)\n",
    "c1_3 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1_2)\n",
    "c1_4 = tf.keras.layers.Conv2D(48, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1_3)\n",
    "c1_5 = tf.keras.layers.Conv2D(48, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1_4)\n",
    "bt1 = tf.keras.layers.BatchNormalization()(c1_5)\n",
    "p1 = tf.keras.layers.MaxPooling2D((2, 2))(bt1)\n",
    "dr1 = tf.keras.layers.Dropout(0.5)(p1)\n",
    "\n",
    "\n",
    "c2_1 = tf.keras.layers.Conv2D(80, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(dr1)\n",
    "c2_2 = tf.keras.layers.Conv2D(80, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2_1)\n",
    "c2_3 = tf.keras.layers.Conv2D(80, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2_2)\n",
    "c2_4 = tf.keras.layers.Conv2D(80, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2_3)\n",
    "c2_5 = tf.keras.layers.Conv2D(80, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2_4)\n",
    "bt2 = tf.keras.layers.BatchNormalization()(c2_5)\n",
    "p2 = tf.keras.layers.MaxPooling2D((2, 2))(bt2)\n",
    "dr2 = tf.keras.layers.Dropout(0.5)(p2)\n",
    "\n",
    "c3_1 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(dr2)\n",
    "c3_2 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3_1)\n",
    "c3_3 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3_2)\n",
    "c3_4 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3_3)\n",
    "c3_5 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3_4)\n",
    "bt3 = tf.keras.layers.BatchNormalization()(c3_5)\n",
    "p3 = tf.keras.layers.MaxPooling2D((8, 8))(bt3)\n",
    "dr3 = tf.keras.layers.Dropout(0.5)(p3)\n",
    "flatten1=tf.keras.layers.Flatten()(dr3)\n",
    "\n",
    "ED = ds_layer.DS1(prototypes,128)(flatten1)\n",
    "ED_ac = ds_layer.DS1_activate(prototypes)(ED)\n",
    "mass_prototypes = ds_layer.DS2(prototypes, num_class)(ED_ac)\n",
    "mass_prototypes_omega = ds_layer.DS2_omega(prototypes, num_class)(mass_prototypes)\n",
    "mass_Dempster = ds_layer.DS3_Dempster(prototypes, num_class)(mass_prototypes_omega)\n",
    "mass_Dempster_normalize = ds_layer.DS3_normalize()(mass_Dempster)\n",
    "\n",
    "outputs = utility_layer_train.DM(0.9, num_class)(mass_Dempster_normalize)\n",
    "\n",
    "\n",
    "model_e = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
    "model_e.compile(optimizer=keras.optimizers.Nadam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004), \n",
    "              loss='CategoricalCrossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model_e.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2313c02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201/201 [==============================] - 1s 2ms/step\n",
      "63/63 [==============================] - 0s 4ms/step\n",
      "Epoch 1/2\n",
      "257/257 [==============================] - 86s 172ms/step - loss: 0.9438 - accuracy: 0.6600 - val_loss: 0.9180 - val_accuracy: 0.6520\n",
      "Epoch 2/2\n",
      "257/257 [==============================] - 40s 155ms/step - loss: 0.7427 - accuracy: 0.7461 - val_loss: 0.8475 - val_accuracy: 0.7209\n",
      "Epoch 1/3\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.8367 - accuracy: 0.7265\n",
      "Epoch 1: val_accuracy improved from -inf to 0.71842, saving model to /Python/BDSL49/Recognition_Processed_Compressed/DatasetNPY32x32\\evidential_DL_200_checkpoint\n",
      "257/257 [==============================] - 94s 188ms/step - loss: 0.8367 - accuracy: 0.7265 - val_loss: 0.9141 - val_accuracy: 0.7184\n",
      "Epoch 2/3\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.7672 - accuracy: 0.7391\n",
      "Epoch 2: val_accuracy did not improve from 0.71842\n",
      "257/257 [==============================] - 44s 170ms/step - loss: 0.7672 - accuracy: 0.7391 - val_loss: 0.9994 - val_accuracy: 0.6860\n",
      "Epoch 3/3\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.7143 - accuracy: 0.7578\n",
      "Epoch 3: val_accuracy did not improve from 0.71842\n",
      "257/257 [==============================] - 43s 166ms/step - loss: 0.7143 - accuracy: 0.7578 - val_loss: 1.2850 - val_accuracy: 0.7004\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26aa1ce16d8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_e.load_weights('/Python/BDSL49/Recognition_Processed_Compressed/DatasetNPY32x32/cnn_checkpoint_fina')\n",
    "feature = tf.keras.Model(inputs=[inputs], outputs=[flatten1])\n",
    "x_train_feature = feature.predict(train_images)\n",
    "x_test_feature = feature.predict(test_images)\n",
    "\n",
    "inputs = tf.keras.layers.Input(128)\n",
    "ED = ds_layer.DS1(prototypes,128)(inputs)\n",
    "ED_ac = ds_layer.DS1_activate(prototypes)(ED)\n",
    "mass_prototypes = ds_layer.DS2(prototypes, num_class)(ED_ac)\n",
    "mass_prototypes_omega = ds_layer.DS2_omega(prototypes, num_class)(mass_prototypes)\n",
    "mass_Dempster = ds_layer.DS3_Dempster(prototypes, num_class)(mass_prototypes_omega)\n",
    "mass_Dempster_normalize = ds_layer.DS3_normalize()(mass_Dempster)\n",
    "outputs = utility_layer_train.DM(0.9, num_class)(mass_Dempster_normalize)\n",
    "model_mid = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
    "model_mid.compile(optimizer=keras.optimizers.Nadam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004), #0.001\n",
    "              loss='CategoricalCrossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model_mid.fit(x_train_feature, train_labels_one_hot, batch_size=25,  epochs=2, verbose=1, validation_data=(x_test_feature, test_labels_one_hot), shuffle=True)\n",
    "\n",
    "\n",
    "model_e.load_weights('/Python/BDSL49/Recognition_Processed_Compressed/DatasetNPY32x32/cnn_checkpoint_fina')\n",
    "DSLAYER_DS1_W = tf.reshape(model_mid.layers[1].get_weights()[0], [1, 200, 128])\n",
    "DSLAYER_DS1_activate_W = model_mid.layers[2].get_weights()\n",
    "DSLAYER_DS2_W = model_mid.layers[3].get_weights()\n",
    "model_e.layers[26].set_weights(DSLAYER_DS1_W)\n",
    "model_e.layers[27].set_weights(DSLAYER_DS1_activate_W)\n",
    "model_e.layers[28].set_weights(DSLAYER_DS2_W)\n",
    "\n",
    "\n",
    "filepath = '/Python/BDSL49/Recognition_Processed_Compressed/DatasetNPY32x32/evidential_DL_200_checkpoint'\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath, monitor='val_accuracy', verbose=1,\n",
    "    save_best_only=True, save_weights_only=True,\n",
    "    save_frequency=1)\n",
    "model_e.fit(train_images, train_labels_one_hot, batch_size=25,  epochs=3, verbose=1, callbacks=[checkpoint_callback], validation_data=(test_images, test_labels_one_hot), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1cf5139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257/257 [==============================] - 9s 35ms/step - loss: 0.7615 - accuracy: 0.7532\n",
      "81/81 [==============================] - 3s 33ms/step - loss: 0.9141 - accuracy: 0.7184\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9140844345092773, 0.7184223532676697]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_e.load_weights('/Python/BDSL49/Recognition_Processed_Compressed/DatasetNPY32x32/evidential_DL_200_checkpoint')\n",
    "model_e.evaluate(train_images, train_labels_one_hot, batch_size=25, verbose=1)\n",
    "model_e.evaluate(test_images, test_labels_one_hot, batch_size=25, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46b3fa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_class=num_classes\n",
    "def func(x):\n",
    "  fun=0\n",
    "  for i in range(len(x)):\n",
    "    fun += x[i] * math.log10(x[i])\n",
    "  return fun\n",
    "\n",
    "def cons1(x):\n",
    "  return sum(x)\n",
    "\n",
    "def cons2(x):\n",
    "  tol = 0\n",
    "  for i in range(len(x)):\n",
    "    tol += (len(x) -(i+1)) * x[i] / (len(x) - 1)\n",
    "  return tol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "811757af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -0.30102999566398114\n",
      "            Iterations: 2\n",
      "            Function evaluations: 6\n",
      "            Gradient evaluations: 2\n",
      "[0.5 0.5]\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -0.29228525323862886\n",
      "            Iterations: 2\n",
      "            Function evaluations: 6\n",
      "            Gradient evaluations: 2\n",
      "[0.6 0.4]\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -0.2652949955741215\n",
      "            Iterations: 2\n",
      "            Function evaluations: 6\n",
      "            Gradient evaluations: 2\n",
      "[0.7 0.3]\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -0.21732201127364892\n",
      "            Iterations: 2\n",
      "            Function evaluations: 6\n",
      "            Gradient evaluations: 2\n",
      "[0.8 0.2]\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -0.14118174150460777\n",
      "            Iterations: 2\n",
      "            Function evaluations: 6\n",
      "            Gradient evaluations: 2\n",
      "[0.9 0.1]\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -0.4771212535458546\n",
      "            Iterations: 5\n",
      "            Function evaluations: 20\n",
      "            Gradient evaluations: 5\n",
      "[0.33335066 0.33329867 0.33335066]\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -0.4639930045670594\n",
      "            Iterations: 5\n",
      "            Function evaluations: 20\n",
      "            Gradient evaluations: 5\n",
      "[0.43838115 0.3232377  0.23838115]\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -0.42332558406764637\n",
      "            Iterations: 4\n",
      "            Function evaluations: 16\n",
      "            Gradient evaluations: 4\n",
      "[0.55389806 0.29220388 0.15389806]\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -0.3504192036873859\n",
      "            Iterations: 3\n",
      "            Function evaluations: 13\n",
      "            Gradient evaluations: 3\n",
      "[0.68188629 0.23622741 0.08188629]\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -0.23258769536137028\n",
      "            Iterations: 6\n",
      "            Function evaluations: 26\n",
      "            Gradient evaluations: 6\n",
      "[0.82626492 0.14747015 0.02626492]\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -0.6020599847597952\n",
      "            Iterations: 6\n",
      "            Function evaluations: 31\n",
      "            Gradient evaluations: 6\n",
      "[0.25004509 0.24995132 0.2499621  0.2500415 ]\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -0.5862953559285206\n",
      "            Iterations: 6\n",
      "            Function evaluations: 31\n",
      "            Gradient evaluations: 6\n",
      "[0.34733484 0.27246999 0.21305548 0.16713968]\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -0.5373185874307769\n",
      "            Iterations: 6\n",
      "            Function evaluations: 32\n",
      "            Gradient evaluations: 6\n",
      "[0.46135577 0.27564645 0.16463979 0.09835799]\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -0.4488881356507039\n",
      "            Iterations: 7\n",
      "            Function evaluations: 37\n",
      "            Gradient evaluations: 7\n",
      "[0.59646995 0.25205466 0.10648084 0.04499455]\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -0.30367594655248775\n",
      "            Iterations: 9\n",
      "            Function evaluations: 51\n",
      "            Gradient evaluations: 9\n",
      "[0.76420482 0.18192697 0.0435316  0.01033661]\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -0.6989699832116538\n",
      "            Iterations: 6\n",
      "            Function evaluations: 39\n",
      "            Gradient evaluations: 6\n",
      "[0.20005948 0.19994333 0.1999245  0.20008306 0.19998962]\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -0.6814447613701302\n",
      "            Iterations: 8\n",
      "            Function evaluations: 49\n",
      "            Gradient evaluations: 8\n",
      "[0.28844499 0.23529717 0.19188048 0.15656759 0.12780977]\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -0.6268738134921664\n",
      "            Iterations: 5\n",
      "            Function evaluations: 34\n",
      "            Gradient evaluations: 5\n",
      "[0.39619071 0.25741801 0.16717569 0.10863174 0.07058384]\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -0.5277827570117529\n",
      "            Iterations: 6\n",
      "            Function evaluations: 40\n",
      "            Gradient evaluations: 6\n",
      "[0.53067919 0.25648282 0.12396    0.05991479 0.0289632 ]\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -0.3628892810901947\n",
      "            Iterations: 10\n",
      "            Function evaluations: 67\n",
      "            Gradient evaluations: 10\n",
      "[0.71040833 0.20691294 0.06004662 0.01753459 0.00509751]\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -0.7781512089028868\n",
      "            Iterations: 6\n",
      "            Function evaluations: 45\n",
      "            Gradient evaluations: 6\n",
      "[0.16663792 0.16670998 0.16672558 0.16662589 0.166542   0.16675864]\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -0.7593665951055877\n",
      "            Iterations: 8\n",
      "            Function evaluations: 59\n",
      "            Gradient evaluations: 8\n",
      "[0.24677183 0.20730733 0.17396845 0.14616515 0.12267586 0.10311137]\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -0.7007693651403221\n",
      "            Iterations: 10\n",
      "            Function evaluations: 75\n",
      "            Gradient evaluations: 10\n",
      "[0.34747342 0.23978387 0.1654592  0.11417385 0.07877212 0.05433754]\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -0.5938824811121751\n",
      "            Iterations: 12\n",
      "            Function evaluations: 90\n",
      "            Gradient evaluations: 12\n",
      "[0.4781138  0.25477167 0.13569587 0.07233782 0.03858102 0.02049981]\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -0.4140336451797974\n",
      "            Iterations: 11\n",
      "            Function evaluations: 87\n",
      "            Gradient evaluations: 11\n",
      "[0.66371979 0.22376807 0.07557901 0.02559504 0.00840165 0.00293644]\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -0.8450979753125784\n",
      "            Iterations: 13\n",
      "            Function evaluations: 109\n",
      "            Gradient evaluations: 13\n",
      "[0.14286257 0.14275642 0.14296659 0.142941   0.14278645 0.1427802\n",
      " 0.14290676]\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -0.8253673875460833\n",
      "            Iterations: 11\n",
      "            Function evaluations: 91\n",
      "            Gradient evaluations: 11\n",
      "[0.21540031 0.18536763 0.15925899 0.1365197  0.11690686 0.10035121\n",
      " 0.0861953 ]\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -0.76373210162451\n",
      "            Iterations: 12\n",
      "            Function evaluations: 101\n",
      "            Gradient evaluations: 12\n",
      "[0.30964064 0.2234583  0.16146283 0.11685142 0.08382624 0.06080659\n",
      " 0.04395398]\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -0.6508864016151569\n",
      "            Iterations: 16\n",
      "            Function evaluations: 139\n",
      "            Gradient evaluations: 16\n",
      "[0.43535828 0.24938697 0.14301937 0.08262447 0.04694849 0.02706763\n",
      " 0.01559479]\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -0.4592351357309824\n",
      "            Iterations: 14\n",
      "            Function evaluations: 125\n",
      "            Gradient evaluations: 14\n",
      "[0.62277325 0.23539618 0.08873893 0.0336843  0.01276929 0.0048324\n",
      " 0.00180565]\n"
     ]
    }
   ],
   "source": [
    "for j in range(2,(num_class+1)):\n",
    "  num_weights = j\n",
    "  ini_weights = np.asarray(np.random.rand(num_weights))\n",
    "\n",
    "  name='weight'+str(j)\n",
    "  locals()['weight'+str(j)]= np.zeros([5, j])\n",
    "\n",
    "  for i in range(5):\n",
    "    tol = 0.5 + i * 0.1\n",
    "\n",
    "    cons = ({'type': 'eq', 'fun' : lambda x: cons1(x)-1},\n",
    "          {'type': 'eq', 'fun' : lambda x: cons2(x)-tol},\n",
    "          {'type': 'ineq', 'fun' : lambda x: x-0.00000001}\n",
    "        )\n",
    "  \n",
    "    res = minimize(func, ini_weights, method='SLSQP', options={'disp': True}, constraints=cons)\n",
    "    locals()['weight'+str(j)][i] = res.x\n",
    "    print (res.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "467f0fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PowerSetsBinary(items):  \n",
    "    N = len(items)   \n",
    "    set_all=[]\n",
    "    for i in range(2**N):\n",
    "        combo = []  \n",
    "        for j in range(N):  \n",
    "            if(i >> j ) % 2 == 1:  \n",
    "                combo.append(items[j]) \n",
    "        set_all.append(combo)\n",
    "    return set_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cacab01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0], [0, 1], [0, 1, 2], [0, 1, 2, 3], [0, 1, 2, 3, 4], [0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5, 6], [0, 1, 2, 3, 4, 6], [0, 1, 2, 3, 5], [0, 1, 2, 3, 5, 6], [0, 1, 2, 3, 6], [0, 1, 2, 4], [0, 1, 2, 4, 5], [0, 1, 2, 4, 5, 6], [0, 1, 2, 4, 6], [0, 1, 2, 5], [0, 1, 2, 5, 6], [0, 1, 2, 6], [0, 1, 3], [0, 1, 3, 4], [0, 1, 3, 4, 5], [0, 1, 3, 4, 5, 6], [0, 1, 3, 4, 6], [0, 1, 3, 5], [0, 1, 3, 5, 6], [0, 1, 3, 6], [0, 1, 4], [0, 1, 4, 5], [0, 1, 4, 5, 6], [0, 1, 4, 6], [0, 1, 5], [0, 1, 5, 6], [0, 1, 6], [0, 2], [0, 2, 3], [0, 2, 3, 4], [0, 2, 3, 4, 5], [0, 2, 3, 4, 5, 6], [0, 2, 3, 4, 6], [0, 2, 3, 5], [0, 2, 3, 5, 6], [0, 2, 3, 6], [0, 2, 4], [0, 2, 4, 5], [0, 2, 4, 5, 6], [0, 2, 4, 6], [0, 2, 5], [0, 2, 5, 6], [0, 2, 6], [0, 3], [0, 3, 4], [0, 3, 4, 5], [0, 3, 4, 5, 6], [0, 3, 4, 6], [0, 3, 5], [0, 3, 5, 6], [0, 3, 6], [0, 4], [0, 4, 5], [0, 4, 5, 6], [0, 4, 6], [0, 5], [0, 5, 6], [0, 6], [1], [1, 2], [1, 2, 3], [1, 2, 3, 4], [1, 2, 3, 4, 5], [1, 2, 3, 4, 5, 6], [1, 2, 3, 4, 6], [1, 2, 3, 5], [1, 2, 3, 5, 6], [1, 2, 3, 6], [1, 2, 4], [1, 2, 4, 5], [1, 2, 4, 5, 6], [1, 2, 4, 6], [1, 2, 5], [1, 2, 5, 6], [1, 2, 6], [1, 3], [1, 3, 4], [1, 3, 4, 5], [1, 3, 4, 5, 6], [1, 3, 4, 6], [1, 3, 5], [1, 3, 5, 6], [1, 3, 6], [1, 4], [1, 4, 5], [1, 4, 5, 6], [1, 4, 6], [1, 5], [1, 5, 6], [1, 6], [2], [2, 3], [2, 3, 4], [2, 3, 4, 5], [2, 3, 4, 5, 6], [2, 3, 4, 6], [2, 3, 5], [2, 3, 5, 6], [2, 3, 6], [2, 4], [2, 4, 5], [2, 4, 5, 6], [2, 4, 6], [2, 5], [2, 5, 6], [2, 6], [3], [3, 4], [3, 4, 5], [3, 4, 5, 6], [3, 4, 6], [3, 5], [3, 5, 6], [3, 6], [4], [4, 5], [4, 5, 6], [4, 6], [5], [5, 6], [6]]\n",
      "127\n"
     ]
    }
   ],
   "source": [
    "class_set=list(range(num_class))\n",
    "act_set= PowerSetsBinary(class_set)\n",
    "act_set.remove(act_set[0])#emptyset is not needed\n",
    "act_set=sorted(act_set)\n",
    "print(act_set)\n",
    "print(len(act_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac636b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.8        0.8        0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.68188629 0.68188629 0.68188629 0.         0.         0.\n",
      "  0.        ]\n",
      " [0.59646995 0.59646995 0.59646995 0.59646995 0.         0.\n",
      "  0.        ]\n",
      " [0.53067919 0.53067919 0.53067919 0.53067919 0.53067919 0.\n",
      "  0.        ]\n",
      " [0.4781138  0.4781138  0.4781138  0.4781138  0.4781138  0.4781138\n",
      "  0.        ]\n",
      " [0.43535828 0.43535828 0.43535828 0.43535828 0.43535828 0.43535828\n",
      "  0.43535828]\n",
      " [0.4781138  0.4781138  0.4781138  0.4781138  0.4781138  0.\n",
      "  0.4781138 ]\n",
      " [0.53067919 0.53067919 0.53067919 0.53067919 0.         0.53067919\n",
      "  0.        ]\n",
      " [0.4781138  0.4781138  0.4781138  0.4781138  0.         0.4781138\n",
      "  0.4781138 ]\n",
      " [0.53067919 0.53067919 0.53067919 0.53067919 0.         0.\n",
      "  0.53067919]\n",
      " [0.59646995 0.59646995 0.59646995 0.         0.59646995 0.\n",
      "  0.        ]\n",
      " [0.53067919 0.53067919 0.53067919 0.         0.53067919 0.53067919\n",
      "  0.        ]\n",
      " [0.4781138  0.4781138  0.4781138  0.         0.4781138  0.4781138\n",
      "  0.4781138 ]\n",
      " [0.53067919 0.53067919 0.53067919 0.         0.53067919 0.\n",
      "  0.53067919]\n",
      " [0.59646995 0.59646995 0.59646995 0.         0.         0.59646995\n",
      "  0.        ]\n",
      " [0.53067919 0.53067919 0.53067919 0.         0.         0.53067919\n",
      "  0.53067919]\n",
      " [0.59646995 0.59646995 0.59646995 0.         0.         0.\n",
      "  0.59646995]\n",
      " [0.68188629 0.68188629 0.         0.68188629 0.         0.\n",
      "  0.        ]\n",
      " [0.59646995 0.59646995 0.         0.59646995 0.59646995 0.\n",
      "  0.        ]\n",
      " [0.53067919 0.53067919 0.         0.53067919 0.53067919 0.53067919\n",
      "  0.        ]\n",
      " [0.4781138  0.4781138  0.         0.4781138  0.4781138  0.4781138\n",
      "  0.4781138 ]\n",
      " [0.53067919 0.53067919 0.         0.53067919 0.53067919 0.\n",
      "  0.53067919]\n",
      " [0.59646995 0.59646995 0.         0.59646995 0.         0.59646995\n",
      "  0.        ]\n",
      " [0.53067919 0.53067919 0.         0.53067919 0.         0.53067919\n",
      "  0.53067919]\n",
      " [0.59646995 0.59646995 0.         0.59646995 0.         0.\n",
      "  0.59646995]\n",
      " [0.68188629 0.68188629 0.         0.         0.68188629 0.\n",
      "  0.        ]\n",
      " [0.59646995 0.59646995 0.         0.         0.59646995 0.59646995\n",
      "  0.        ]\n",
      " [0.53067919 0.53067919 0.         0.         0.53067919 0.53067919\n",
      "  0.53067919]\n",
      " [0.59646995 0.59646995 0.         0.         0.59646995 0.\n",
      "  0.59646995]\n",
      " [0.68188629 0.68188629 0.         0.         0.         0.68188629\n",
      "  0.        ]\n",
      " [0.59646995 0.59646995 0.         0.         0.         0.59646995\n",
      "  0.59646995]\n",
      " [0.68188629 0.68188629 0.         0.         0.         0.\n",
      "  0.68188629]\n",
      " [0.8        0.         0.8        0.         0.         0.\n",
      "  0.        ]\n",
      " [0.68188629 0.         0.68188629 0.68188629 0.         0.\n",
      "  0.        ]\n",
      " [0.59646995 0.         0.59646995 0.59646995 0.59646995 0.\n",
      "  0.        ]\n",
      " [0.53067919 0.         0.53067919 0.53067919 0.53067919 0.53067919\n",
      "  0.        ]\n",
      " [0.4781138  0.         0.4781138  0.4781138  0.4781138  0.4781138\n",
      "  0.4781138 ]\n",
      " [0.53067919 0.         0.53067919 0.53067919 0.53067919 0.\n",
      "  0.53067919]\n",
      " [0.59646995 0.         0.59646995 0.59646995 0.         0.59646995\n",
      "  0.        ]\n",
      " [0.53067919 0.         0.53067919 0.53067919 0.         0.53067919\n",
      "  0.53067919]\n",
      " [0.59646995 0.         0.59646995 0.59646995 0.         0.\n",
      "  0.59646995]\n",
      " [0.68188629 0.         0.68188629 0.         0.68188629 0.\n",
      "  0.        ]\n",
      " [0.59646995 0.         0.59646995 0.         0.59646995 0.59646995\n",
      "  0.        ]\n",
      " [0.53067919 0.         0.53067919 0.         0.53067919 0.53067919\n",
      "  0.53067919]\n",
      " [0.59646995 0.         0.59646995 0.         0.59646995 0.\n",
      "  0.59646995]\n",
      " [0.68188629 0.         0.68188629 0.         0.         0.68188629\n",
      "  0.        ]\n",
      " [0.59646995 0.         0.59646995 0.         0.         0.59646995\n",
      "  0.59646995]\n",
      " [0.68188629 0.         0.68188629 0.         0.         0.\n",
      "  0.68188629]\n",
      " [0.8        0.         0.         0.8        0.         0.\n",
      "  0.        ]\n",
      " [0.68188629 0.         0.         0.68188629 0.68188629 0.\n",
      "  0.        ]\n",
      " [0.59646995 0.         0.         0.59646995 0.59646995 0.59646995\n",
      "  0.        ]\n",
      " [0.53067919 0.         0.         0.53067919 0.53067919 0.53067919\n",
      "  0.53067919]\n",
      " [0.59646995 0.         0.         0.59646995 0.59646995 0.\n",
      "  0.59646995]\n",
      " [0.68188629 0.         0.         0.68188629 0.         0.68188629\n",
      "  0.        ]\n",
      " [0.59646995 0.         0.         0.59646995 0.         0.59646995\n",
      "  0.59646995]\n",
      " [0.68188629 0.         0.         0.68188629 0.         0.\n",
      "  0.68188629]\n",
      " [0.8        0.         0.         0.         0.8        0.\n",
      "  0.        ]\n",
      " [0.68188629 0.         0.         0.         0.68188629 0.68188629\n",
      "  0.        ]\n",
      " [0.59646995 0.         0.         0.         0.59646995 0.59646995\n",
      "  0.59646995]\n",
      " [0.68188629 0.         0.         0.         0.68188629 0.\n",
      "  0.68188629]\n",
      " [0.8        0.         0.         0.         0.         0.8\n",
      "  0.        ]\n",
      " [0.68188629 0.         0.         0.         0.         0.68188629\n",
      "  0.68188629]\n",
      " [0.8        0.         0.         0.         0.         0.\n",
      "  0.8       ]\n",
      " [0.         1.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.8        0.8        0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.68188629 0.68188629 0.68188629 0.         0.\n",
      "  0.        ]\n",
      " [0.         0.59646995 0.59646995 0.59646995 0.59646995 0.\n",
      "  0.        ]\n",
      " [0.         0.53067919 0.53067919 0.53067919 0.53067919 0.53067919\n",
      "  0.        ]\n",
      " [0.         0.4781138  0.4781138  0.4781138  0.4781138  0.4781138\n",
      "  0.4781138 ]\n",
      " [0.         0.53067919 0.53067919 0.53067919 0.53067919 0.\n",
      "  0.53067919]\n",
      " [0.         0.59646995 0.59646995 0.59646995 0.         0.59646995\n",
      "  0.        ]\n",
      " [0.         0.53067919 0.53067919 0.53067919 0.         0.53067919\n",
      "  0.53067919]\n",
      " [0.         0.59646995 0.59646995 0.59646995 0.         0.\n",
      "  0.59646995]\n",
      " [0.         0.68188629 0.68188629 0.         0.68188629 0.\n",
      "  0.        ]\n",
      " [0.         0.59646995 0.59646995 0.         0.59646995 0.59646995\n",
      "  0.        ]\n",
      " [0.         0.53067919 0.53067919 0.         0.53067919 0.53067919\n",
      "  0.53067919]\n",
      " [0.         0.59646995 0.59646995 0.         0.59646995 0.\n",
      "  0.59646995]\n",
      " [0.         0.68188629 0.68188629 0.         0.         0.68188629\n",
      "  0.        ]\n",
      " [0.         0.59646995 0.59646995 0.         0.         0.59646995\n",
      "  0.59646995]\n",
      " [0.         0.68188629 0.68188629 0.         0.         0.\n",
      "  0.68188629]\n",
      " [0.         0.8        0.         0.8        0.         0.\n",
      "  0.        ]\n",
      " [0.         0.68188629 0.         0.68188629 0.68188629 0.\n",
      "  0.        ]\n",
      " [0.         0.59646995 0.         0.59646995 0.59646995 0.59646995\n",
      "  0.        ]\n",
      " [0.         0.53067919 0.         0.53067919 0.53067919 0.53067919\n",
      "  0.53067919]\n",
      " [0.         0.59646995 0.         0.59646995 0.59646995 0.\n",
      "  0.59646995]\n",
      " [0.         0.68188629 0.         0.68188629 0.         0.68188629\n",
      "  0.        ]\n",
      " [0.         0.59646995 0.         0.59646995 0.         0.59646995\n",
      "  0.59646995]\n",
      " [0.         0.68188629 0.         0.68188629 0.         0.\n",
      "  0.68188629]\n",
      " [0.         0.8        0.         0.         0.8        0.\n",
      "  0.        ]\n",
      " [0.         0.68188629 0.         0.         0.68188629 0.68188629\n",
      "  0.        ]\n",
      " [0.         0.59646995 0.         0.         0.59646995 0.59646995\n",
      "  0.59646995]\n",
      " [0.         0.68188629 0.         0.         0.68188629 0.\n",
      "  0.68188629]\n",
      " [0.         0.8        0.         0.         0.         0.8\n",
      "  0.        ]\n",
      " [0.         0.68188629 0.         0.         0.         0.68188629\n",
      "  0.68188629]\n",
      " [0.         0.8        0.         0.         0.         0.\n",
      "  0.8       ]\n",
      " [0.         0.         1.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.8        0.8        0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.68188629 0.68188629 0.68188629 0.\n",
      "  0.        ]\n",
      " [0.         0.         0.59646995 0.59646995 0.59646995 0.59646995\n",
      "  0.        ]\n",
      " [0.         0.         0.53067919 0.53067919 0.53067919 0.53067919\n",
      "  0.53067919]\n",
      " [0.         0.         0.59646995 0.59646995 0.59646995 0.\n",
      "  0.59646995]\n",
      " [0.         0.         0.68188629 0.68188629 0.         0.68188629\n",
      "  0.        ]\n",
      " [0.         0.         0.59646995 0.59646995 0.         0.59646995\n",
      "  0.59646995]\n",
      " [0.         0.         0.68188629 0.68188629 0.         0.\n",
      "  0.68188629]\n",
      " [0.         0.         0.8        0.         0.8        0.\n",
      "  0.        ]\n",
      " [0.         0.         0.68188629 0.         0.68188629 0.68188629\n",
      "  0.        ]\n",
      " [0.         0.         0.59646995 0.         0.59646995 0.59646995\n",
      "  0.59646995]\n",
      " [0.         0.         0.68188629 0.         0.68188629 0.\n",
      "  0.68188629]\n",
      " [0.         0.         0.8        0.         0.         0.8\n",
      "  0.        ]\n",
      " [0.         0.         0.68188629 0.         0.         0.68188629\n",
      "  0.68188629]\n",
      " [0.         0.         0.8        0.         0.         0.\n",
      "  0.8       ]\n",
      " [0.         0.         0.         1.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.8        0.8        0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.68188629 0.68188629 0.68188629\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.59646995 0.59646995 0.59646995\n",
      "  0.59646995]\n",
      " [0.         0.         0.         0.68188629 0.68188629 0.\n",
      "  0.68188629]\n",
      " [0.         0.         0.         0.8        0.         0.8\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.68188629 0.         0.68188629\n",
      "  0.68188629]\n",
      " [0.         0.         0.         0.8        0.         0.\n",
      "  0.8       ]\n",
      " [0.         0.         0.         0.         1.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.8        0.8\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.68188629 0.68188629\n",
      "  0.68188629]\n",
      " [0.         0.         0.         0.         0.8        0.\n",
      "  0.8       ]\n",
      " [0.         0.         0.         0.         0.         1.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.8\n",
      "  0.8       ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "utility_matrix = np.zeros([len(act_set), len(class_set)])\n",
    "tol_i = 3 \n",
    "for i in range(len(act_set)):\n",
    "  intersec = class_set and act_set[i]\n",
    "  if len(intersec) == 1:\n",
    "    utility_matrix[i, intersec] = 1\n",
    "  \n",
    "  else:\n",
    "    for j in range(len(intersec)):\n",
    "      utility_matrix[i, intersec[j]] = locals()['weight'+str(len(intersec))][tol_i, 0]\n",
    "print (utility_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "47961d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " conv2d_30 (Conv2D)          (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " conv2d_31 (Conv2D)          (None, 32, 32, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_32 (Conv2D)          (None, 32, 32, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_33 (Conv2D)          (None, 32, 32, 48)        13872     \n",
      "                                                                 \n",
      " conv2d_34 (Conv2D)          (None, 32, 32, 48)        20784     \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 32, 32, 48)       192       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 16, 16, 48)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 16, 16, 48)        0         \n",
      "                                                                 \n",
      " conv2d_35 (Conv2D)          (None, 16, 16, 80)        34640     \n",
      "                                                                 \n",
      " conv2d_36 (Conv2D)          (None, 16, 16, 80)        57680     \n",
      "                                                                 \n",
      " conv2d_37 (Conv2D)          (None, 16, 16, 80)        57680     \n",
      "                                                                 \n",
      " conv2d_38 (Conv2D)          (None, 16, 16, 80)        57680     \n",
      "                                                                 \n",
      " conv2d_39 (Conv2D)          (None, 16, 16, 80)        57680     \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 16, 16, 80)       320       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 8, 8, 80)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 8, 8, 80)          0         \n",
      "                                                                 \n",
      " conv2d_40 (Conv2D)          (None, 8, 8, 128)         92288     \n",
      "                                                                 \n",
      " conv2d_41 (Conv2D)          (None, 8, 8, 128)         147584    \n",
      "                                                                 \n",
      " conv2d_42 (Conv2D)          (None, 8, 8, 128)         147584    \n",
      "                                                                 \n",
      " conv2d_43 (Conv2D)          (None, 8, 8, 128)         147584    \n",
      "                                                                 \n",
      " conv2d_44 (Conv2D)          (None, 8, 8, 128)         147584    \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 8, 8, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 1, 1, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 1, 1, 128)         0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 128)               0         \n",
      "                                                                 \n",
      " ds1_2 (DS1)                 (None, 200)               25600     \n",
      "                                                                 \n",
      " ds1_activate_2 (DS1_activat  (None, 200)              400       \n",
      " e)                                                              \n",
      "                                                                 \n",
      " ds2_2 (DS2)                 (None, 200, 7)            1400      \n",
      "                                                                 \n",
      " ds2_omega_2 (DS2_omega)     (None, 200, 8)            0         \n",
      "                                                                 \n",
      " ds3__dempster_2 (DS3_Dempst  (None, 8)                0         \n",
      " er)                                                             \n",
      "                                                                 \n",
      " ds3_normalize_2 (DS3_normal  (None, 8)                0         \n",
      " ize)                                                            \n",
      "                                                                 \n",
      " dm_test (DM_test)           (None, 127)               889       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,031,345\n",
      "Trainable params: 1,029,944\n",
      "Non-trainable params: 1,401\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prototypes=200\n",
    "number_act_set = len(act_set)\n",
    "\n",
    "inputs = tf.keras.layers.Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
    "\n",
    "\n",
    "c1_1 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(inputs)\n",
    "c1_2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1_1)\n",
    "c1_3 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1_2)\n",
    "c1_4 = tf.keras.layers.Conv2D(48, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1_3)\n",
    "c1_5 = tf.keras.layers.Conv2D(48, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1_4)\n",
    "bt1 = tf.keras.layers.BatchNormalization()(c1_5)\n",
    "p1 = tf.keras.layers.MaxPooling2D((2, 2))(bt1)\n",
    "dr1 = tf.keras.layers.Dropout(0.5)(p1)\n",
    "\n",
    "\n",
    "c2_1 = tf.keras.layers.Conv2D(80, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(dr1)\n",
    "c2_2 = tf.keras.layers.Conv2D(80, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2_1)\n",
    "c2_3 = tf.keras.layers.Conv2D(80, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2_2)\n",
    "c2_4 = tf.keras.layers.Conv2D(80, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2_3)\n",
    "c2_5 = tf.keras.layers.Conv2D(80, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2_4)\n",
    "bt2 = tf.keras.layers.BatchNormalization()(c2_5)\n",
    "p2 = tf.keras.layers.MaxPooling2D((2, 2))(bt2)\n",
    "dr2 = tf.keras.layers.Dropout(0.5)(p2)\n",
    "\n",
    "c3_1 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(dr2)\n",
    "c3_2 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3_1)\n",
    "c3_3 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3_2)\n",
    "c3_4 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3_3)\n",
    "c3_5 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3_4)\n",
    "bt3 = tf.keras.layers.BatchNormalization()(c3_5)\n",
    "p3 = tf.keras.layers.MaxPooling2D((8, 8))(bt3)\n",
    "dr3 = tf.keras.layers.Dropout(0.5)(p3)\n",
    "flatten1=tf.keras.layers.Flatten()(dr3)\n",
    "\n",
    "#DS layer\n",
    "ED = ds_layer.DS1(prototypes,128)(flatten1)\n",
    "ED_ac = ds_layer.DS1_activate(prototypes)(ED)\n",
    "mass_prototypes = ds_layer.DS2(prototypes, num_class)(ED_ac)\n",
    "mass_prototypes_omega = ds_layer.DS2_omega(prototypes, num_class)(mass_prototypes)\n",
    "mass_Dempster = ds_layer.DS3_Dempster(prototypes, num_class)(mass_prototypes_omega)\n",
    "mass_Dempster_normalize = ds_layer.DS3_normalize()(mass_Dempster)\n",
    "\n",
    "#Utility layer for testing\n",
    "outputs = utility_layer_test.DM_test(num_class, number_act_set, 0.9)(mass_Dempster_normalize)\n",
    "\n",
    "\n",
    "model_e_imprecise = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
    "model_e_imprecise.compile(optimizer=keras.optimizers.Nadam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004), \n",
    "              loss='CategoricalCrossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model_e_imprecise.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4851b306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x26aeec47fd0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_e_imprecise.layers[-1].set_weights(tf.reshape(utility_matrix, [1, 127, 7]))\n",
    "model_e_imprecise.load_weights('/Python/BDSL49/Recognition_Processed_Compressed/DatasetNPY32x32/evidential_DL_200_checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2ab35f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 3s 48ms/step\n",
      "[[0], [3, 4], [2], [0], [0], [0], [0], [2], [0], [0], [0], [0, 1, 2], [1], [0, 1, 2], [0], [0], [0], [0], [0, 3], [0], [0, 2], [0, 1, 2, 3], [1, 2], [0, 2, 3], [0, 1, 2, 3], [0, 1, 2], [0, 3], [2], [0, 1, 3, 4], [0, 1, 2], [0], [0], [0], [0], [0], [0], [0], [0], [0], [1, 2], [0], [0], [0], [0], [2], [0, 3], [0], [0], [0], [0], [0, 1, 2], [0], [0], [0], [0], [3, 4], [0], [0, 1, 3], [0, 3, 4], [0], [0, 2, 3], [0], [0, 2], [1, 2], [0], [3, 4], [0, 1, 2, 3, 4], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0, 1], [0], [0, 1], [3, 4], [0, 2], [0], [0], [0], [2], [0], [0], [0], [3, 4], [0, 1, 2], [0], [3, 4], [0], [0], [0], [0], [0, 2, 3, 4], [0], [0], [0], [2], [0], [0], [0], [0, 1, 2], [0, 1, 2], [0], [0, 3, 4], [0], [0, 2], [0], [0, 2], [0], [0], [0], [0], [0, 2, 3, 4], [0], [0], [0], [0], [0], [0], [0], [0, 1, 2], [0], [0, 2], [0], [0], [3, 4], [0], [0], [0, 1], [0], [0, 1], [0], [2, 3, 4], [2], [0], [0], [3, 4], [0, 1], [0], [0, 2, 3], [0], [0], [0], [0], [0, 1, 2], [2], [0, 2], [0], [0], [0, 2], [0], [0, 1, 3], [0], [0], [0], [0], [0, 2, 3], [0, 2, 3], [0], [0], [0], [2], [0], [0], [0, 1, 2], [0, 3, 4], [0], [0], [0], [0, 2], [3, 4], [1, 2, 3], [0], [0, 2, 3], [0], [0], [0], [3, 4], [2, 3, 4], [0], [3, 4], [0], [0, 2], [0], [0], [0], [0], [0], [0], [0], [0], [3, 4], [0, 2], [0, 3, 4], [0], [2], [0], [2], [0], [0], [0], [0, 2, 3], [0], [0], [0], [0], [0], [1], [0], [0], [0], [0, 2], [0], [0], [0], [0, 2], [0], [0], [0], [0, 2], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0, 2, 3, 4], [0], [0, 1, 3, 4], [0, 1], [0], [0], [0], [3, 4], [0], [3, 4], [0], [0], [0], [0], [0], [0, 3, 4], [0], [0], [0, 3, 4], [0], [0], [3, 4], [1, 2], [0], [3, 4], [0], [0], [3, 4], [0], [0], [0], [0], [0, 2, 3, 4], [0], [0], [0], [0], [0], [0], [0], [0], [0, 1, 2, 3], [0, 3, 4], [0], [0], [0], [1, 2, 3], [0], [0, 1], [1], [2, 3, 4], [0], [0], [0], [0], [0], [0], [0], [0, 2], [0], [0], [1, 2], [0], [0], [0], [0], [0], [0, 1, 2], [0], [0], [0], [0, 2], [0], [3, 4], [0, 1, 2, 3, 4], [0], [3, 4], [0, 3], [0], [0], [0], [0], [0, 3, 4], [0], [0, 1], [0], [0], [0], [0], [2], [0], [0], [0], [0], [0, 1, 2], [0, 1, 2], [0], [0, 3, 4], [0, 1, 2], [0], [0, 3, 4], [0], [0], [3, 4], [0], [3, 4], [0], [0], [0], [0], [3, 4], [0], [2], [0, 2, 3, 4], [1, 2, 3], [0], [0, 3, 4], [3, 4], [0], [3, 4], [0, 1, 2], [3, 4], [0], [0], [0], [2], [0, 2, 3, 4], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [3, 4], [0], [0], [0], [0], [0], [0], [0], [0], [2], [1, 2], [0], [0], [0], [2], [1, 2], [0, 3, 4], [3, 4], [0, 3, 4], [0], [0, 3, 4], [0], [0], [0], [0, 2], [0], [0], [0], [0], [0, 1, 2, 3], [3, 4], [0], [0, 2], [0], [0], [0], [0, 1, 2, 3], [0], [0], [0], [0], [2], [0], [0], [0], [0], [0, 1, 2], [0, 1, 2], [0, 2], [0], [0, 2, 3], [0], [0, 3, 4], [0], [0], [0, 1, 2], [0], [0], [0], [0, 2, 3, 4], [2], [0], [3, 4], [0, 1, 2], [0], [0], [0, 1, 2, 3], [0], [0], [0], [2, 3, 4], [0], [0, 3], [0, 1, 2], [0, 3], [0], [0], [0, 2, 3, 4], [0], [0], [0, 1, 2], [2], [0], [0], [0], [0], [0, 2, 3, 4], [0, 1], [0], [1, 2], [0], [0], [0, 3, 4], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0, 2], [0], [0, 2, 3, 4], [0], [0, 1, 3], [0], [0], [3, 4], [3, 4], [0], [0], [0], [0], [0], [0], [0], [0], [3, 4], [0, 3, 4], [0], [3, 4], [0], [0], [0, 1], [0], [0], [0], [0], [0, 3, 4], [0], [0], [0], [0], [0], [3, 4], [3, 4], [0], [3, 4], [3, 4], [0], [2], [0, 1], [0], [0], [0], [0], [0, 1, 2, 3], [0, 1, 3], [0], [0], [3, 4], [0], [0], [0], [0], [0], [1, 2], [0], [0, 3, 4], [0], [0], [0], [0], [0, 2], [0], [0], [0], [0], [0], [0, 1, 2, 3], [0], [0, 3], [0], [0], [0], [0], [0, 2], [2], [0], [0, 2], [0], [0], [0, 2], [0], [0], [0, 1], [0], [0], [3, 4], [0], [0], [0], [0, 3, 4], [0], [0], [0], [3, 4], [0], [0, 1], [0], [0], [0, 2, 3, 4], [2, 3, 4], [1], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [3, 4], [0], [0, 3], [0], [0], [0], [0, 2], [0], [1, 2], [0], [0], [0, 1], [0], [0], [0], [0], [0, 3], [0], [3, 4], [0, 1], [0], [3, 4], [0], [0, 2, 3, 4], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0, 1, 2], [0], [0], [0], [0], [0], [0], [0, 3, 4], [0], [0, 2, 3], [0], [0], [0], [3, 4], [0], [0], [0], [0], [0], [3, 4], [0, 2, 3, 4], [0], [0], [0], [3, 4], [0], [0], [2], [0], [0], [0], [0, 2], [0], [3, 4], [3, 4], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0, 3], [2], [0], [0], [0], [0], [0], [0, 3, 4], [0], [0], [0], [0], [0, 1, 2], [0], [0], [1], [0], [0], [0], [0], [0], [3, 4], [0], [0], [0, 2], [3, 4], [0], [0], [0], [2], [0], [0, 1], [0, 1], [2, 3, 4], [0], [0, 3, 4], [3, 4], [0, 3], [0], [0], [0, 1, 2], [2], [1, 2], [0], [0], [0], [0], [0], [0], [0], [0], [0, 3, 4], [0, 3, 4], [0], [0], [0], [0], [0], [3, 4], [0], [0, 2, 3, 4], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [3, 4], [2], [0], [3, 4], [0], [0], [0], [0], [0], [0], [2, 3, 4], [0, 1, 3, 4], [3, 4], [0, 1], [3, 4], [0, 1], [0], [2], [0], [0], [0], [1, 3], [0, 1, 2], [3, 4], [0, 1, 2], [0], [0], [0], [3, 4], [0], [2, 3, 4], [0], [0], [3, 4], [0, 1, 2, 3], [0], [0], [0], [0], [0], [3, 4], [0], [0, 3, 4], [0], [0], [0], [0], [0], [2], [1, 2], [0], [0, 1, 2], [0], [0, 1, 2], [0], [1, 2], [0], [0, 3], [0, 1, 3], [0], [3, 4], [0], [0, 2], [0], [0], [0, 2], [0, 1, 2, 3, 4], [0], [0], [0], [2], [0], [2], [0, 1, 2, 3], [0], [3, 4], [0, 2], [2, 3, 4], [0, 2, 3, 4], [0], [0], [0], [0], [1], [0], [0, 3, 4], [3, 4], [0], [0, 3, 4], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [3, 4], [0], [0], [0], [0, 1], [0], [3, 4], [0, 3], [3, 4], [0], [0], [0], [0, 3, 4], [0], [0], [0], [3, 4], [0], [3, 4], [0, 1, 2, 3], [0], [3, 4], [0], [0], [0], [1, 2], [0], [0], [0, 1], [0], [0], [0, 1, 2], [3, 4], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [3, 4], [0], [0, 3, 4], [0], [0], [0], [0, 1], [0], [0], [0], [0], [0, 1], [1], [0], [0], [1, 2], [0], [3, 4], [0, 2], [0], [0], [0], [1, 2], [0], [0, 1, 2], [0, 2], [3, 4], [3, 4], [0], [0], [0], [0], [0], [3, 4], [0], [0, 3, 4], [0], [0], [0], [0], [3, 4], [0, 1], [0], [0], [0], [0], [0], [0], [0], [0], [0, 1], [3, 4], [3, 4], [0, 1, 2], [0], [0], [3, 4], [0], [0], [0, 3], [0], [3, 4], [0], [0], [0, 2, 3, 4], [0], [0], [0], [0], [0, 1], [0], [0], [0], [0], [0], [0], [2], [0, 2, 3, 4], [0, 3], [0], [0], [0], [0, 1, 3], [3, 4], [3, 4], [0], [0], [0], [0, 2], [0], [0], [0], [0], [0], [0, 1, 2], [0], [0], [2], [0], [0, 1], [3, 4], [0], [0], [1], [0, 2, 3], [0, 1], [3, 4], [1], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [2], [3, 4], [0], [0, 1], [0], [0], [0], [0], [0], [2], [0, 2], [0], [0, 2, 3], [0, 2], [0], [0, 3, 4], [0], [0], [0], [0, 1, 2], [3, 4], [3, 4], [0], [0], [0], [0], [3, 4], [0, 2, 3], [0], [0], [0], [0], [0], [0], [0], [1], [2, 3], [0], [0, 2], [0], [3, 4], [0], [0], [0], [0], [0], [1, 2], [0], [2, 3, 4], [0], [0], [0], [3, 4], [1, 2], [0], [0], [0], [0], [0], [3, 4], [2], [0], [0], [0], [0], [0], [0], [0], [0], [3, 4], [0, 1, 2, 3], [0], [1], [0, 1, 2], [0], [0], [0], [0], [0, 1, 2], [0], [2], [0], [0, 3, 4], [0], [3, 4], [3, 4], [2], [0, 1, 2], [2], [2, 3, 4], [0, 3, 4], [0], [0], [0], [0], [0], [0, 2, 3], [0], [0], [0, 3], [0], [0], [0, 2, 3, 4], [3, 4], [0, 1], [0, 1, 2], [1], [0], [0], [0, 1], [0], [0, 1, 2, 3], [0], [0], [3, 4], [0], [0], [0], [3, 4], [0], [0, 1], [3, 4], [0, 1], [0], [0], [3, 4], [0, 3, 4], [0, 3], [0, 2, 3], [0], [0], [0], [0, 1, 2], [0], [0], [0, 2, 3, 4], [0, 2], [0], [3, 4], [0, 3, 4], [0], [3, 4], [0, 1, 2, 3], [1], [0], [0], [3, 4], [0], [0], [0], [0], [0], [0], [0, 1], [1], [3, 4], [0], [3, 4], [0], [0, 2, 3], [0], [0], [0, 2, 3, 4], [0], [3, 4], [0], [0, 1, 2], [0, 3, 4], [3, 4], [3, 4], [0], [0], [0], [0, 2, 3, 4], [0], [0], [0], [1, 2, 3, 4], [0], [0], [0], [1, 2], [0], [0], [0], [0, 1], [0], [0], [0], [0, 3, 4], [0, 2, 3], [0], [1, 2], [0], [0], [0], [0, 2, 3], [0, 2], [0], [0], [0], [0], [0], [3, 4], [0], [0], [0], [0], [0], [0], [0], [3, 4], [2], [0], [3, 4], [0], [0], [3, 4], [0], [0], [1, 2], [0], [0], [0], [0, 3, 4], [0, 2, 3], [2, 3, 4], [0, 3, 4], [0], [0], [2], [1], [0], [0], [0], [0], [0], [0, 3], [0], [3, 4], [0], [0], [2, 3, 4], [0, 2, 3, 4], [0], [0], [0], [0], [0, 3, 4], [0], [0, 1, 2, 3], [0], [0, 1], [0], [0], [3, 4], [0], [2], [0], [0], [0], [0], [2, 3, 4], [0, 1], [0], [3, 4], [3, 4], [0], [0, 2], [0], [1, 2], [0], [0, 1, 2], [0], [0], [0], [3, 4], [0], [0], [2], [0], [3, 4], [1, 2], [0], [0], [0], [3, 4], [0], [0], [2, 3, 4], [0], [0], [0], [0, 1], [0], [0], [0], [0], [0], [2], [0], [0], [3, 4], [0], [0, 3, 4], [0], [0], [0], [0], [0], [0], [0, 2], [0, 1, 2, 3], [0], [3, 4], [0], [0], [0], [1], [0], [0], [0], [0], [0, 1, 2], [0], [1], [0, 2], [0], [3, 4], [0], [0, 2, 3], [0, 1, 2], [1, 2], [0, 1], [0], [0, 2, 3, 4], [0], [0, 3, 4], [0], [0], [0, 2], [0], [0], [1, 2], [0, 1, 2, 3, 4], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0, 3, 4], [0], [0, 2, 3], [0], [0, 2], [0], [0, 2], [2], [0], [0], [0], [0], [0], [0, 2], [0], [0], [2, 3, 4], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [3, 4], [0], [0, 2, 3, 4], [0, 3, 4], [0], [0], [3, 4], [0], [0], [0, 2], [0], [0, 3, 4], [0, 1], [0], [0, 3], [3, 4], [0, 2, 3], [0, 3, 4], [0], [0], [0], [0], [0, 1, 2], [0], [0], [2], [0], [0], [0], [2, 3, 4], [3, 4], [0], [0], [0, 3, 4], [0], [0], [0], [1], [3, 4], [3, 4], [2], [0], [0], [0], [0], [0], [0, 1, 2, 3, 4], [2], [0], [0], [0, 1, 2, 3], [0, 1, 3, 4], [3, 4], [0, 1, 2], [0], [0], [0], [0], [0], [0, 3, 4], [2], [0, 3, 4], [3, 4], [0], [0], [0], [2], [0], [0], [0], [0], [0, 2], [0], [0, 1, 2], [0], [0], [0], [3, 4], [0], [0], [0, 1, 3], [0], [0], [3, 4], [0], [0], [0], [0], [0], [1, 2], [1, 2], [0], [0], [0], [3, 4], [0], [0], [0], [0], [0], [0], [0, 1, 2, 3], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0, 2], [0], [3, 4], [0, 1], [0, 2], [1, 2], [0, 3], [0], [0, 2, 3], [0], [0], [0], [0], [3, 4], [0], [0], [0], [3, 4], [0], [0], [0, 3, 4], [3, 4], [0], [0], [0], [0], [0], [0, 2, 3, 4], [0], [0, 3, 4], [1], [0], [0], [1, 2], [0], [3, 4], [0], [3, 4], [0], [0], [0], [0], [0], [0], [3, 4], [0, 3, 4], [0], [0], [0], [0], [3, 4], [0], [0], [0, 3, 4], [0], [1, 2], [0], [0], [0, 1, 2], [0], [0], [0, 1], [0], [0, 1, 3], [0, 1, 2], [0, 3], [0, 1, 3], [1], [3, 4], [0], [0], [0], [0], [0], [0], [0], [0], [0, 2], [0, 3], [0], [0], [0, 1], [3, 4], [0, 2], [0], [0], [0], [0, 2], [0], [3, 4], [0], [2, 3, 4], [0], [0], [0], [0], [0], [0], [2, 3], [0], [0], [0, 3, 4], [0], [1, 2], [0, 3, 4], [0, 2, 3], [3, 4], [3, 4], [0], [0, 3], [0, 1, 2], [0], [0], [0], [0], [0], [2], [0], [3, 4], [0], [0], [3, 4], [3, 4], [1], [0], [3, 4], [0], [0], [0], [0], [3, 4], [0], [0], [0], [0], [0], [0, 1, 2, 3], [0], [0, 3], [0, 3, 4], [3, 4], [0, 1, 2], [0], [3, 4], [0], [2], [0], [0], [0], [0], [3, 4], [0], [0], [0], [0], [0], [0, 3, 4], [0], [0], [0], [0], [0, 3, 4], [0], [0], [0, 3, 4], [0], [3, 4], [0], [0], [0], [0], [0], [0], [2], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0, 1], [0], [3, 4], [0], [0, 3], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0, 3], [0, 2, 3], [0], [0], [0], [0], [0], [0], [0], [3, 4], [0, 3, 4], [0, 1, 2], [0], [0], [0, 3], [0], [0], [0], [3, 4], [0], [0], [0], [0], [0], [0, 1, 2], [0], [0, 1], [0], [0], [0, 1, 3], [3, 4], [0], [0, 1, 2], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0, 1], [3, 4], [3, 4], [0], [0], [0], [0], [0], [0], [0], [0], [3, 4], [0], [3, 4], [0], [0], [0], [0], [1, 2], [2], [0, 3, 4], [0], [2, 3, 4], [0], [0], [0], [0], [0], [1, 2], [0, 3, 4], [0], [0], [2, 3, 4], [0, 2, 3], [2], [0], [0], [3, 4], [0], [0, 1, 2], [0, 3, 4], [0], [0], [3, 4], [0], [0], [0], [0], [0], [0, 3, 4], [0], [3, 4], [0], [0], [2, 3, 4], [0, 3, 4], [0], [0], [0], [0, 2], [0, 1, 2], [2, 3, 4], [2], [0], [0], [3, 4], [0], [0], [2, 3, 4], [0], [0], [0, 1], [0], [0], [0], [0], [0, 1, 2, 3], [0], [0, 1, 2, 3], [0], [0, 2], [0], [0], [0], [0], [0, 1, 2], [2], [0, 1], [0], [2], [0, 1, 2], [0], [0], [0, 1, 2], [0], [0], [0], [0], [0, 1, 2, 3, 4], [0], [0], [3, 4], [0], [3, 4], [0], [0], [0, 1, 2, 3], [0], [0, 1], [0], [3, 4], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [2], [0], [0], [0], [3, 4], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0, 3, 4], [0], [1, 2], [3, 4], [0], [0], [0, 3, 4], [0], [1, 2], [3, 4], [0], [0, 3, 4], [0], [0], [0], [0, 2, 3], [0, 3, 4], [1, 2], [0], [2], [0], [3, 4], [0], [0, 2], [0, 2], [0], [0], [0], [0, 2, 3], [0], [1], [0], [1, 2], [0, 2], [0], [0, 2, 3, 4], [0], [3, 4], [0], [0], [0, 3, 4], [0], [0], [0], [0, 1], [0], [0, 1, 2, 3, 4], [0], [0], [0], [0], [0], [0, 3], [0], [0], [0], [1, 2, 3], [0], [2], [0], [0, 3, 4], [0, 2, 3], [0, 1, 3], [3, 4], [0, 1], [0], [0], [0], [0], [3, 4], [0, 1, 2], [0, 1, 2, 3], [0], [0, 2], [0], [0, 3, 4], [0], [0], [3, 4], [0], [0], [0, 3, 4], [0], [0], [0, 1], [0], [0], [0], [1, 2, 3, 4], [0], [2], [0], [0], [0, 3, 4], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0, 2], [0, 1, 2, 3], [0], [0], [0], [0], [0, 3, 4]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.75908831]\n"
     ]
    }
   ],
   "source": [
    "resutls = tf.argmax(model_e_imprecise.predict(test_images),-1)\n",
    "imprecise_results =[]\n",
    "for i in range(len(resutls)):\n",
    "  act_local = resutls[i]\n",
    "  set_valued_results = act_set[act_local]\n",
    "  imprecise_results.append(set_valued_results)\n",
    "print (imprecise_results)\n",
    "average_utility_imprecision = AU_imprecision.average_utility(utility_matrix, resutls, test_labels, act_set)\n",
    "print (average_utility_imprecision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77835b08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-gpu",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
